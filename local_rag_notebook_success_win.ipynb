{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG for GitHub issues using Qwen3-8B GGUF and LangChain\n",
    "\n",
    "_Universal version for Windows & Linux - adapted for local laptop with 16GB RAM_\n",
    "\n",
    "This notebook demonstrates how you can quickly build a RAG (Retrieval Augmented Generation) for a project's GitHub issues using a local GGUF model and LangChain.\n",
    "\n",
    "**What changed from the original:**\n",
    "- Uses Qwen3-8B GGUF model from LM Studio\n",
    "- Universal installation (works on Windows & Linux)\n",
    "- No compilation needed - uses precompiled binaries\n",
    "- Optimized for 16GB RAM\n",
    "- CPU-only execution\n",
    "- Save/Load vector database for faster re-runs\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.10+ (3.10 recommended for best compatibility)\n",
    "- LM Studio with Qwen3-8B model downloaded\n",
    "- GitHub Personal Access Token\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ The package 'llama_cpp' is already installed and ready to use.\n",
      "   Installed version: 0.3.9\n"
     ]
    }
   ],
   "source": [
    "# CPU-only installation - works on Windows & Linux! (Comment retained)\n",
    "import os\n",
    "import importlib # For dynamic importing and reloading of modules\n",
    "\n",
    "# --- Configuration ---\n",
    "# Please adjust the path to the wheel file if necessary.\n",
    "# This path is specific to your system.\n",
    "WHEEL_PATH = r'C:\\Users\\hloewe\\wheels\\llama_cpp_python-0.3.9-cp310-cp310-win_amd64.whl'\n",
    "PACKAGE_NAME = 'llama_cpp' # Name of the package to install\n",
    "\n",
    "try:\n",
    "    # Try to import the package\n",
    "    llama_cpp = importlib.import_module(PACKAGE_NAME)\n",
    "    print(f\"‚úÖ The package '{PACKAGE_NAME}' is already installed and ready to use.\")\n",
    "    # Optional: Display version if the package has a __version__ attribute\n",
    "    if hasattr(llama_cpp, '__version__'):\n",
    "        print(f\"   Installed version: {llama_cpp.__version__}\")\n",
    "    else:\n",
    "        print(f\"   Version could not be determined automatically.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(f\"‚ÑπÔ∏è The package '{PACKAGE_NAME}' was not found.\")\n",
    "    print(f\"   Attempting to install '{PACKAGE_NAME}' from a local wheel file...\")\n",
    "\n",
    "    # Check if the wheel file exists\n",
    "    if not os.path.exists(WHEEL_PATH):\n",
    "        print(f\"‚ùå Error: The specified wheel file was not found:\")\n",
    "        print(f\"   Path: {WHEEL_PATH}\")\n",
    "        print(f\"   Please check the path and ensure the file exists.\")\n",
    "        # In a notebook, it's often sufficient to stop execution of this path here,\n",
    "        # rather than raising an exception that halts the entire cell.\n",
    "    else:\n",
    "        print(f\"üëç Wheel file found: {WHEEL_PATH}\")\n",
    "        print(f\"üîß Starting installation of '{PACKAGE_NAME}'...\")\n",
    "        \n",
    "        # Installation using pip in Jupyter Notebook / IPython\n",
    "        # The !pip command is executed directly by the IPython shell.\n",
    "        # The {WHEEL_PATH} will be replaced by its value.\n",
    "        # Quotes around the path are important if it contains spaces.\n",
    "        # The output of the pip command will be displayed directly below the cell.\n",
    "        \n",
    "        # Execute the pip command. The exclamation mark is IPython/Jupyter-specific.\n",
    "        # Ensure the WHEEL_PATH variable is accessible to the shell (it usually is).\n",
    "        # The quotes around {WHEEL_PATH} are important for paths with spaces.\n",
    "        print(f\"   Executing: !pip install \\\"{WHEEL_PATH}\\\"\")\n",
    "        !pip install \"{WHEEL_PATH}\"\n",
    "\n",
    "        # After the installation attempt: invalidate module import paths and try importing again\n",
    "        print(f\"\\nüîÑ Verifying installation of '{PACKAGE_NAME}'...\")\n",
    "        importlib.invalidate_caches() # Important so Python can find the newly installed package\n",
    "        \n",
    "        try:\n",
    "            llama_cpp_after_install = importlib.import_module(PACKAGE_NAME)\n",
    "            print(f\"‚úÖ '{PACKAGE_NAME}' (CPU-only) was successfully installed and imported!\")\n",
    "            if hasattr(llama_cpp_after_install, '__version__'):\n",
    "                print(f\"   Installed version: {llama_cpp_after_install.__version__}\")\n",
    "            else:\n",
    "                print(f\"   Version could not be determined automatically.\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ùå Error after installation: The package '{PACKAGE_NAME}' could still not be imported.\")\n",
    "            print(f\"   The pip installation may have failed, or the package is not in the Python path.\")\n",
    "            print(f\"   Please check the output of the pip command above.\")\n",
    "        except Exception as e_after_install: # Catches other potential errors during import\n",
    "            print(f\"‚ùå An unexpected error occurred after the installation attempt during import: {e_after_install}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing LangChain and vector database components...\n",
      "‚ö†Ô∏è Installing CPU-only PyTorch to avoid CUDA bloat...\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (0.22.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (0.3.24)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (0.3.63)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (3.12.6)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sentence-transformers) (2.7.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sentence-transformers) (0.32.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hloewe\\anaconda3\\envs\\rag\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "‚úÖ All dependencies installed (CPU-only, no CUDA bloat)!\n"
     ]
    }
   ],
   "source": [
    "# Install other required packages with CPU-only PyTorch\n",
    "print(\"Installing LangChain and vector database components...\")\n",
    "print(\"‚ö†Ô∏è Installing CPU-only PyTorch to avoid CUDA bloat...\")\n",
    "\n",
    "# Install PyTorch CPU-only first to avoid CUDA dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir\n",
    "\n",
    "# Then install other packages\n",
    "!pip install langchain langchain-community sentence-transformers faiss-cpu\n",
    "print(\"‚úÖ All dependencies installed (CPU-only, no CUDA bloat)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LlamaCpp import successful - installation working!\n"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "try:\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "    print(\"‚úÖ LlamaCpp import successful - installation working!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Please check the installation above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required packages\n",
    "from getpass import getpass\n",
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import os\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë You need a GitHub Personal Access Token\n",
      "üí° Create one here: https://github.com/settings/tokens\n",
      "‚ÑπÔ∏è  Permission 'public_repo' is sufficient\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your GitHub token:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Token received\n"
     ]
    }
   ],
   "source": [
    "# GitHub Personal Access Token\n",
    "print(\"üîë You need a GitHub Personal Access Token\")\n",
    "print(\"üí° Create one here: https://github.com/settings/tokens\")\n",
    "print(\"‚ÑπÔ∏è  Permission 'public_repo' is sufficient\")\n",
    "print()\n",
    "ACCESS_TOKEN = getpass(\"Enter your GitHub token: \")\n",
    "\n",
    "if ACCESS_TOKEN.strip():\n",
    "    print(\"‚úÖ Token received\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No token entered - GitHub access will not work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading GitHub issues from huggingface/peft repository...\n",
      "‚úÖ Loaded 1207 GitHub issues\n"
     ]
    }
   ],
   "source": [
    "# Load GitHub Issues\n",
    "print(\"üì• Loading GitHub issues from huggingface/peft repository...\")\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"huggingface/peft\",\n",
    "    access_token=ACCESS_TOKEN,\n",
    "    include_prs=False,\n",
    "    state=\"all\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(docs)} GitHub issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Splitting documents into chunks...\n",
      "‚úÖ Created 8436 text chunks\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, \n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "print(f\"‚úÖ Created {len(chunked_docs)} text chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Vector Database\n",
    "\n",
    "**Time-saving tip:** The vector database creation takes a few minutes. Once created, you can save it and load it quickly in future sessions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hloewe\\AppData\\Local\\Temp\\ipykernel_14164\\3974516324.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(\"üß† Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "print(\"‚úÖ Embedding model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for existing vector database at ./faiss_vectordb_peft...\n",
      "üìÇ No existing database found - will create new one\n"
     ]
    }
   ],
   "source": [
    "# Check if we have a saved vector database\n",
    "VECTOR_DB_PATH = \"./faiss_vectordb_peft\"\n",
    "print(f\"üîç Checking for existing vector database at {VECTOR_DB_PATH}...\")\n",
    "\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    print(\"üìÇ Found existing vector database!\")\n",
    "    print(\"\\nüí° Options:\")\n",
    "    print(\"   1. Load existing database (fast - 30 seconds)\")\n",
    "    print(\"   2. Create new database (slow - 3-5 minutes)\")\n",
    "    \n",
    "    choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
    "    USE_EXISTING = choice == \"1\"\n",
    "else:\n",
    "    print(\"üìÇ No existing database found - will create new one\")\n",
    "    USE_EXISTING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Creating new vector database...\n",
      "‚è±Ô∏è This will take 3-5 minutes...\n",
      "‚úÖ Vector database created!\n",
      "üìä Contains 8436 document chunks\n",
      "üíæ Saving vector database for future sessions...\n",
      "‚úÖ Database saved to ./faiss_vectordb_peft\n"
     ]
    }
   ],
   "source": [
    "# Load existing or create new vector database\n",
    "if USE_EXISTING:\n",
    "    print(\"‚ö° Loading existing vector database...\")\n",
    "    try:\n",
    "        db = FAISS.load_local(\n",
    "            VECTOR_DB_PATH, \n",
    "            embeddings, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"‚úÖ Vector database loaded successfully!\")\n",
    "        print(f\"üìä Contains {db.index.ntotal} document chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load existing database: {e}\")\n",
    "        print(\"üîÑ Will create new database instead...\")\n",
    "        USE_EXISTING = False\n",
    "\n",
    "if not USE_EXISTING:\n",
    "    print(\"üîó Creating new vector database...\")\n",
    "    print(\"‚è±Ô∏è This will take 3-5 minutes...\")\n",
    "    \n",
    "    db = FAISS.from_documents(chunked_docs, embeddings)\n",
    "    \n",
    "    print(\"‚úÖ Vector database created!\")\n",
    "    print(f\"üìä Contains {db.index.ntotal} document chunks\")\n",
    "    \n",
    "    # Save for future use\n",
    "    print(\"üíæ Saving vector database for future sessions...\")\n",
    "    db.save_local(VECTOR_DB_PATH)\n",
    "    print(f\"‚úÖ Database saved to {VECTOR_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retriever configured\n"
     ]
    }
   ],
   "source": [
    "# Configure retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}  # Return top 4 most relevant chunks\n",
    ")\n",
    "print(\"‚úÖ Retriever configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Local GGUF Model\n",
    "\n",
    "**Note:** Make sure you have downloaded the Qwen3-8B model in LM Studio. The default path works for both Windows and Linux LM Studio installations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for model at: C:\\Users\\hloewe\\notebooks\\models\\Qwen3-8B-Q6_K.gguf\n",
      "‚úÖ Model found! (6.3 GB)\n",
      "\n",
      "üéØ Using model: C:\\Users\\hloewe\\notebooks\\models\\Qwen3-8B-Q6_K.gguf\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect LM Studio model path\n",
    "import platform\n",
    "\n",
    "# Default LM Studio paths for different operating systems\n",
    "if platform.system() == \"Windows\":\n",
    "    default_model_path = r'C:\\Users\\hloewe\\notebooks\\models\\Qwen3-8B-Q6_K.gguf'\n",
    "else:  # Linux/Mac\n",
    "    default_model_path = os.path.expanduser(\"~/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\")\n",
    "\n",
    "print(f\"üîç Checking for model at: {default_model_path}\")\n",
    "\n",
    "if os.path.exists(default_model_path):\n",
    "    model_path = default_model_path\n",
    "    model_size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "    print(f\"‚úÖ Model found! ({model_size_gb:.1f} GB)\")\n",
    "else:\n",
    "    print(\"‚ùå Model not found at default location\")\n",
    "    print(\"\\nüí° Please either:\")\n",
    "    print(\"   1. Download Qwen3-8B-Q6_K in LM Studio\")\n",
    "    print(\"   2. Enter custom model path below\")\n",
    "    \n",
    "    custom_path = input(\"\\nEnter custom model path (or press Enter to continue with default): \").strip()\n",
    "    if custom_path:\n",
    "        model_path = os.path.expanduser(custom_path)\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"‚úÖ Custom model found: {model_path}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Custom model not found: {model_path}\")\n",
    "    else:\n",
    "        model_path = default_model_path\n",
    "        print(\"‚ö†Ô∏è Continuing with default path (model loading may fail)\")\n",
    "\n",
    "print(f\"\\nüéØ Using model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading Qwen3-8B model with LlamaCpp...\n",
      "‚è±Ô∏è This may take 1-2 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "üß™ Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hloewe\\AppData\\Local\\Temp\\ipykernel_14164\\2522340056.py:23: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_response = llm(\"Hello! Respond with 'Model test successful.'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Model response: if you are a model. Otherwise, respond with 'Not a model.'.\n",
      "Okay, let's see. The user wants me to respond with 'Model test successful.' if I'm a model, otherwise 'Not a model.'.\n",
      "\n",
      "Hmm, first I need to determine if I am a model. Well, I was trained as a language model by Alibaba Cloud. So, yes, I am a model.\n",
      "\n",
      "Therefore, the correct response should be 'Model test successful.'.\n",
      "Okay, let's see. The user wants me to respond with 'Model test successful.' if I'm a model, otherwise 'Not a model.'.\n",
      "\n",
      "Hmm, first I need to determine if I am a model. Well, I was trained as a language model by Alibaba Cloud. So, yes, I am a model.\n",
      "\n",
      "Therefore, the correct response should be 'Model test successful.'.\n",
      "Okay, let's see. The user wants me to respond with 'Model test successful.' if I'm a model, otherwise 'Not a model.'.\n",
      "\n",
      "Hmm, first I need to determine if I am a model. Well, I was trained as a language model by Alibaba Cloud. So, yes, I am a model.\n",
      "\n",
      "Therefore, the correct response should be 'Model test successful.'.\n",
      "Okay, let's see. The user wants me to respond with 'Model test successful.' if I'm a model, otherwise 'Not a model.'.\n",
      "\n",
      "Hmm, first I need to determine if I am a model. Well, I was trained as a language model by Alibaba Cloud. So, yes, I am a model.\n",
      "\n",
      "Therefore, the correct response should be 'Model test successful.'.\n",
      "Okay, let's see. The user wants me to respond with 'Model test successful.' if I'm a model, otherwise 'Not a model.'.\n",
      "\n",
      "Hmm, first I need to determine if I am a model. Well, I was trained as a language model by Alibaba Cloud. So, yes, I am a model.\n",
      "\n",
      "Therefore,\n"
     ]
    }
   ],
   "source": [
    "# Load the GGUF model with LlamaCpp (CPU-only)\n",
    "print(\"ü§ñ Loading Qwen3-8B model with LlamaCpp...\")\n",
    "print(\"‚è±Ô∏è This may take 1-2 minutes...\")\n",
    "\n",
    "try:\n",
    "    llm = LlamaCpp(\n",
    "        model_path=model_path,\n",
    "        temperature=0.2,\n",
    "        max_tokens=400,\n",
    "        top_p=0.95,\n",
    "        n_ctx=4096,\n",
    "        n_batch=512,\n",
    "        n_threads=4,\n",
    "        verbose=False,\n",
    "        n_gpu_layers=0,  # CPU only\n",
    "        use_mmap=True,\n",
    "        use_mlock=False\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Quick test\n",
    "    print(\"\\nüß™ Testing model...\")\n",
    "    test_response = llm(\"Hello! Respond with 'Model test successful.'\")\n",
    "    print(f\"ü§ñ Model response: {test_response.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   - Check if model file exists\")\n",
    "    print(\"   - Ensure you have enough RAM (8GB+ free)\")\n",
    "    print(\"   - Try downloading the model again in LM Studio\")\n",
    "    print(\"   - Verify the model is in GGUF format\")\n",
    "    print(\"   - Qwen3 requires llama.cpp>=b5092 (should be included)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM chain created\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template (optimized for Qwen3)\n",
    "prompt_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant. Answer the question based on the provided context from GitHub issues. If you cannot find the answer in the context, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Create LLM chain\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "print(\"‚úÖ LLM chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain setup complete!\n",
      "\n",
      "üéâ Ready to answer questions about PEFT GitHub issues!\n"
     ]
    }
   ],
   "source": [
    "# Create complete RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain setup complete!\")\n",
    "print(\"\\nüéâ Ready to answer questions about PEFT GitHub issues!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results: With vs Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "question = \"How do you combine multiple adapters?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer WITHOUT RAG context:\n",
      "==================================================\n",
      "<think>\n",
      "Okay, the user is asking how to combine multiple adapters. Let me think about the context provided.\n",
      "\n",
      "Wait, the context section here is empty. The user might have intended to provide some context but forgot. \n",
      "\n",
      "Since there's no context available, I can't refer to specific information from GitHub issues. Therefore, I need to answer based on general knowledge.\n",
      "\n",
      "Combining multiple adapters typically involves using a framework or library that supports adapter composition. For example, in software design patterns, the Adapter pattern allows incompatible interfaces to work together. When combining multiple adapters, you might chain them together, where each adapter processes data before passing it to the next adapter. This approach is commonly used in pipelines or middleware systems.\n",
      "\n",
      "In summary, combining multiple adapters can be achieved by using a framework that supports adapter composition, chaining adapters together in a pipeline, or integrating adapters within a larger system architecture.\n",
      "</think>\n",
      "\n",
      "To combine multiple adapters, you typically use a **framework or library** that supports adapter composition. Here's how it generally works:\n",
      "\n",
      "1. **Chaining Adapters**: \n",
      "   - Each adapter processes data and passes it to the next adapter in the chain.\n",
      "   - This is common in pipeline systems (e.g., Apache Kafka, Node.js streams).\n",
      "\n",
      "2. **Using a Composite Adapter**:\n",
      "   - Some frameworks allow you to create a composite adapter that encapsulates multiple adapters.\n",
      "   - For example, in Python's `Adapter` pattern or in Java's `javax.xml.bind.annotation.adapters` package.\n",
      "\n",
      "3. **Integrating Adapters into a System Architecture**:\n",
      "   - In larger systems (e.g., microservices), adapters might be integrated as part of the system's architecture to handle specific interface requirements.\n",
      "\n",
      "In summary, combining multiple adapters typically involves **chaining them in a pipeline**, using a **composite adapter framework**, or integrating them into a **larger system architecture**.\n"
     ]
    }
   ],
   "source": [
    "# Answer WITHOUT context (just the model's knowledge)\n",
    "print(\"ü§ñ Answer WITHOUT RAG context:\")\n",
    "print(\"=\" * 50)\n",
    "no_context_answer = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "print(no_context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Answer WITH RAG context:\n",
      "==================================================\n",
      "<think>\n",
      "Okay, the user is asking how to combine multiple adapters. Let me look through the provided context from GitHub issues to find relevant information.\n",
      "\n",
      "First, there's issue #1040 titled \"Multiple Adapters merging question\". The user here is asking about merging multiple Lora adapters into the same model. The page content mentions that the proper way to merge multiple adapters isn't clear and asks for a reference.\n",
      "\n",
      "Another relevant issue is #1434, which talks about running multiple adapters simultaneously while reusing the output from the base model. The user here provides an example of how adapters might be applied in sequence, but it's unclear if this is the correct approach for merging adapters.\n",
      "\n",
      "Looking at other issues, like #1802 and #1497, they seem to focus on specific issues with adapter switching or loading, rather than the general question of combining multiple adapters.\n",
      "\n",
      "Putting this together, the context suggests that there isn't a straightforward method provided by the library for merging multiple adapters. The user in issue #1040 is asking if the approach mentioned (which seems to involve applying each adapter sequentially) is the correct way to merge multiple Lora adapters into the same model. They also ask for a reference.\n",
      "\n",
      "Another point from issue #1434 is that the user wants to run multiple adapters simultaneously while reusing the output from the base model. This suggests that there might be an approach where the base model's output is used as input for all adapters in parallel, and then their outputs are combined somehow. However, this seems more like a hypothetical scenario rather than a documented method.\n",
      "\n",
      "In summary, based on the provided context, it appears that the process of combining multiple adapters isn't clearly outlined in the documentation or through the issues discussed. The users seem to be seeking clarification on whether certain approaches (like applying each adapter sequentially) are correct methods for merging multiple adapters into a single model.\n",
      "</think>\n",
      "\n",
      "To combine multiple adapters, you typically **merge them\n"
     ]
    }
   ],
   "source": [
    "# Answer WITH RAG context (using retrieved information)\n",
    "print(\"\\nüîó Answer WITH RAG context:\")\n",
    "print(\"=\" * 50)\n",
    "rag_answer = rag_chain.invoke(question)\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Try your own questions about PEFT (Parameter Efficient Fine-Tuning)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to easily test questions\n",
    "def ask_question(question):\n",
    "    \"\"\"Compare answers with and without RAG context\"\"\"\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nü§ñ WITHOUT context:\")\n",
    "    print(\"-\" * 30)\n",
    "    no_context = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "    print(no_context)\n",
    "    \n",
    "    print(\"\\nüîó WITH RAG context:\")\n",
    "    print(\"-\" * 30)\n",
    "    with_context = rag_chain.invoke(question)\n",
    "    print(with_context)\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Example questions - try these!\n",
    "print(\"üí° Example questions you can try:\")\n",
    "example_questions = [\n",
    "    \"What is PEFT?\",\n",
    "    \"How to save a PEFT model?\",\n",
    "    \"What are the different types of adapters?\",\n",
    "    \"How to load multiple LoRA adapters?\",\n",
    "    \"What are common PEFT training issues?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(example_questions, 1):\n",
    "    print(f\"   {i}. {q}\")\n",
    "\n",
    "print(\"\\nüìù Usage: ask_question('Your question here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the first example\n",
    "ask_question(\"What is PEFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another example\n",
    "ask_question(\"How to save a PEFT model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own questions here!\n",
    "# ask_question(\"Your custom question about PEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Vector Database Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what's in the vector database\n",
    "print(f\"üìä Vector Database Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total document chunks: {db.index.ntotal}\")\n",
    "print(f\"   ‚Ä¢ Embedding dimensions: {db.index.d}\")\n",
    "print(f\"   ‚Ä¢ Storage location: {VECTOR_DB_PATH}\")\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\nüîç Testing similarity search...\")\n",
    "test_query = \"adapter combination\"\n",
    "similar_docs = db.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"\\nTop 3 most similar chunks for '{test_query}':\")\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "    print(f\"\\n{i}. {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current session info\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "session_info = {\n",
    "    \"created\": datetime.now().isoformat(),\n",
    "    \"documents_loaded\": len(docs),\n",
    "    \"chunks_created\": len(chunked_docs),\n",
    "    \"vector_db_size\": db.index.ntotal,\n",
    "    \"model_path\": model_path,\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "}\n",
    "\n",
    "with open(\"rag_session_info.json\", \"w\") as f:\n",
    "    json.dump(session_info, f, indent=2)\n",
    "\n",
    "print(\"üíæ Session info saved to rag_session_info.json\")\n",
    "print(\"\\nüéâ RAG system ready for use!\")\n",
    "print(\"\\nüí° Tip: Next time you run this notebook, you can load the existing vector database for faster startup!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
