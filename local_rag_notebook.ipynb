{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple RAG for GitHub issues using Qwen3-8B GGUF and LangChain\n",
        "\n",
        "_Universal version for Windows & Linux - adapted for local laptop with 16GB RAM_\n",
        "\n",
        "This notebook demonstrates how you can quickly build a RAG (Retrieval Augmented Generation) for a project's GitHub issues using a local GGUF model and LangChain.\n",
        "\n",
        "**What changed from the original:**\n",
        "- Uses Qwen3-8B GGUF model from LM Studio\n",
        "- Universal installation (works on Windows & Linux)\n",
        "- No compilation needed - uses precompiled binaries\n",
        "- Optimized for 16GB RAM\n",
        "- CPU-only execution\n",
        "- Save/Load vector database for faster re-runs\n",
        "\n",
        "**Requirements:**\n",
        "- Python 3.10+ (3.10 recommended for best compatibility)\n",
        "- LM Studio with Qwen3-8B model downloaded\n",
        "- GitHub Personal Access Token\n",
        "\n",
        "First, install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CPU-only installation - works on Windows & Linux!\n",
        "print(\"Installing llama-cpp-python (CPU-only, no CUDA dependencies)...\")\n",
        "\n",
        "# Force CPU-only installation to avoid CUDA bloat\n",
        "import os\n",
        "os.environ['CMAKE_ARGS'] = '-DGGML_BLAS=OFF -DGGML_CUDA=OFF -DGGML_METAL=OFF'\n",
        "os.environ['FORCE_CMAKE'] = '1'\n",
        "\n",
        "!pip install llama-cpp-python --no-cache-dir --force-reinstall --no-binary=llama-cpp-python\n",
        "print(\"âœ… llama-cpp-python (CPU-only) installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install other required packages with CPU-only PyTorch\n",
        "print(\"Installing LangChain and vector database components...\")\n",
        "print(\"âš ï¸ Installing CPU-only PyTorch to avoid CUDA bloat...\")\n",
        "\n",
        "# Install PyTorch CPU-only first to avoid CUDA dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir\n",
        "\n",
        "# Then install other packages\n",
        "!pip install langchain langchain-community sentence-transformers faiss-cpu\n",
        "print(\"âœ… All dependencies installed (CPU-only, no CUDA bloat)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "try:\n",
        "    from langchain_community.llms import LlamaCpp\n",
        "    print(\"âœ… LlamaCpp import successful - installation working!\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import failed: {e}\")\n",
        "    print(\"Please check the installation above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required packages\n",
        "from getpass import getpass\n",
        "from langchain.document_loaders import GitHubIssuesLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.llms import LlamaCpp\n",
        "import os\n",
        "print(\"âœ… All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GitHub Personal Access Token\n",
        "print(\"ðŸ”‘ You need a GitHub Personal Access Token\")\n",
        "print(\"ðŸ’¡ Create one here: https://github.com/settings/tokens\")\n",
        "print(\"â„¹ï¸  Permission 'public_repo' is sufficient\")\n",
        "print()\n",
        "ACCESS_TOKEN = getpass(\"Enter your GitHub token: \")\n",
        "\n",
        "if ACCESS_TOKEN.strip():\n",
        "    print(\"âœ… Token received\")\n",
        "else:\n",
        "    print(\"âš ï¸ No token entered - GitHub access will not work\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GitHub Issues\n",
        "print(\"ðŸ“¥ Loading GitHub issues from huggingface/peft repository...\")\n",
        "loader = GitHubIssuesLoader(\n",
        "    repo=\"huggingface/peft\",\n",
        "    access_token=ACCESS_TOKEN,\n",
        "    include_prs=False,\n",
        "    state=\"all\"\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "print(f\"âœ… Loaded {len(docs)} GitHub issues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split documents into chunks\n",
        "print(\"âœ‚ï¸ Splitting documents into chunks...\")\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512, \n",
        "    chunk_overlap=30\n",
        ")\n",
        "\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "print(f\"âœ… Created {len(chunked_docs)} text chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create or Load Vector Database\n",
        "\n",
        "**Time-saving tip:** The vector database creation takes a few minutes. Once created, you can save it and load it quickly in future sessions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize embedding model\n",
        "print(\"ðŸ§  Loading embedding model...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "print(\"âœ… Embedding model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we have a saved vector database\n",
        "VECTOR_DB_PATH = \"./faiss_vectordb_peft\"\n",
        "print(f\"ðŸ” Checking for existing vector database at {VECTOR_DB_PATH}...\")\n",
        "\n",
        "if os.path.exists(VECTOR_DB_PATH):\n",
        "    print(\"ðŸ“‚ Found existing vector database!\")\n",
        "    print(\"\\nðŸ’¡ Options:\")\n",
        "    print(\"   1. Load existing database (fast - 30 seconds)\")\n",
        "    print(\"   2. Create new database (slow - 3-5 minutes)\")\n",
        "    \n",
        "    choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
        "    USE_EXISTING = choice == \"1\"\n",
        "else:\n",
        "    print(\"ðŸ“‚ No existing database found - will create new one\")\n",
        "    USE_EXISTING = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load existing or create new vector database\n",
        "if USE_EXISTING:\n",
        "    print(\"âš¡ Loading existing vector database...\")\n",
        "    try:\n",
        "        db = FAISS.load_local(\n",
        "            VECTOR_DB_PATH, \n",
        "            embeddings, \n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        print(f\"âœ… Vector database loaded successfully!\")\n",
        "        print(f\"ðŸ“Š Contains {db.index.ntotal} document chunks\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to load existing database: {e}\")\n",
        "        print(\"ðŸ”„ Will create new database instead...\")\n",
        "        USE_EXISTING = False\n",
        "\n",
        "if not USE_EXISTING:\n",
        "    print(\"ðŸ”— Creating new vector database...\")\n",
        "    print(\"â±ï¸ This will take 3-5 minutes...\")\n",
        "    \n",
        "    db = FAISS.from_documents(chunked_docs, embeddings)\n",
        "    \n",
        "    print(\"âœ… Vector database created!\")\n",
        "    print(f\"ðŸ“Š Contains {db.index.ntotal} document chunks\")\n",
        "    \n",
        "    # Save for future use\n",
        "    print(\"ðŸ’¾ Saving vector database for future sessions...\")\n",
        "    db.save_local(VECTOR_DB_PATH)\n",
        "    print(f\"âœ… Database saved to {VECTOR_DB_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 4}  # Return top 4 most relevant chunks\n",
        ")\n",
        "print(\"âœ… Retriever configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Local GGUF Model\n",
        "\n",
        "**Note:** Make sure you have downloaded the Qwen3-8B model in LM Studio. The default path works for both Windows and Linux LM Studio installations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-detect LM Studio model path\n",
        "import platform\n",
        "\n",
        "# Default LM Studio paths for different operating systems\n",
        "if platform.system() == \"Windows\":\n",
        "    default_model_path = os.path.expanduser(\"~/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\")\n",
        "else:  # Linux/Mac\n",
        "    default_model_path = os.path.expanduser(\"~/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\")\n",
        "\n",
        "print(f\"ðŸ” Checking for model at: {default_model_path}\")\n",
        "\n",
        "if os.path.exists(default_model_path):\n",
        "    model_path = default_model_path\n",
        "    model_size_gb = os.path.getsize(model_path) / (1024**3)\n",
        "    print(f\"âœ… Model found! ({model_size_gb:.1f} GB)\")\n",
        "else:\n",
        "    print(\"âŒ Model not found at default location\")\n",
        "    print(\"\\nðŸ’¡ Please either:\")\n",
        "    print(\"   1. Download Qwen3-8B-Q6_K in LM Studio\")\n",
        "    print(\"   2. Enter custom model path below\")\n",
        "    \n",
        "    custom_path = input(\"\\nEnter custom model path (or press Enter to continue with default): \").strip()\n",
        "    if custom_path:\n",
        "        model_path = os.path.expanduser(custom_path)\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"âœ… Custom model found: {model_path}\")\n",
        "        else:\n",
        "            print(f\"âŒ Custom model not found: {model_path}\")\n",
        "    else:\n",
        "        model_path = default_model_path\n",
        "        print(\"âš ï¸ Continuing with default path (model loading may fail)\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Using model: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the GGUF model with LlamaCpp (CPU-only)\n",
        "print(\"ðŸ¤– Loading Qwen3-8B model with LlamaCpp...\")\n",
        "print(\"â±ï¸ This may take 1-2 minutes...\")\n",
        "\n",
        "try:\n",
        "    llm = LlamaCpp(\n",
        "        model_path=model_path,\n",
        "        temperature=0.2,\n",
        "        max_tokens=400,\n",
        "        top_p=0.95,\n",
        "        n_ctx=4096,\n",
        "        n_batch=512,\n",
        "        n_threads=4,\n",
        "        verbose=False,\n",
        "        n_gpu_layers=0,  # CPU only\n",
        "        use_mmap=True,\n",
        "        use_mlock=False\n",
        "    )\n",
        "    print(\"âœ… Model loaded successfully!\")\n",
        "    \n",
        "    # Quick test\n",
        "    print(\"\\nðŸ§ª Testing model...\")\n",
        "    test_response = llm(\"Hello! Respond with 'Model test successful.'\")\n",
        "    print(f\"ðŸ¤– Model response: {test_response.strip()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Failed to load model: {e}\")\n",
        "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
        "    print(\"   - Check if model file exists\")\n",
        "    print(\"   - Ensure you have enough RAM (8GB+ free)\")\n",
        "    print(\"   - Try downloading the model again in LM Studio\")\n",
        "    print(\"   - Verify the model is in GGUF format\")\n",
        "    print(\"   - Qwen3 requires llama.cpp>=b5092 (should be included)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup the RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create prompt template (optimized for Qwen3)\n",
        "prompt_template = \"\"\"<|im_start|>system\n",
        "You are a helpful assistant. Answer the question based on the provided context from GitHub issues. If you cannot find the answer in the context, say so clearly.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template\n",
        ")\n",
        "\n",
        "# Create LLM chain\n",
        "llm_chain = prompt | llm | StrOutputParser()\n",
        "print(\"âœ… LLM chain created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create complete RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "print(\"âœ… RAG chain setup complete!\")\n",
        "print(\"\\nðŸŽ‰ Ready to answer questions about PEFT GitHub issues!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Results: With vs Without RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example question\n",
        "question = \"How do you combine multiple adapters?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Answer WITHOUT context (just the model's knowledge)\n",
        "print(\"ðŸ¤– Answer WITHOUT RAG context:\")\n",
        "print(\"=\" * 50)\n",
        "no_context_answer = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
        "print(no_context_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Answer WITH RAG context (using retrieved information)\n",
        "print(\"\\nðŸ”— Answer WITH RAG context:\")\n",
        "print(\"=\" * 50)\n",
        "rag_answer = rag_chain.invoke(question)\n",
        "print(rag_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Testing\n",
        "\n",
        "Try your own questions about PEFT (Parameter Efficient Fine-Tuning)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to easily test questions\n",
        "def ask_question(question):\n",
        "    \"\"\"Compare answers with and without RAG context\"\"\"\n",
        "    print(f\"â“ Question: {question}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(\"\\nðŸ¤– WITHOUT context:\")\n",
        "    print(\"-\" * 30)\n",
        "    no_context = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
        "    print(no_context)\n",
        "    \n",
        "    print(\"\\nðŸ”— WITH RAG context:\")\n",
        "    print(\"-\" * 30)\n",
        "    with_context = rag_chain.invoke(question)\n",
        "    print(with_context)\n",
        "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Example questions - try these!\n",
        "print(\"ðŸ’¡ Example questions you can try:\")\n",
        "example_questions = [\n",
        "    \"What is PEFT?\",\n",
        "    \"How to save a PEFT model?\",\n",
        "    \"What are the different types of adapters?\",\n",
        "    \"How to load multiple LoRA adapters?\",\n",
        "    \"What are common PEFT training issues?\"\n",
        "]\n",
        "\n",
        "for i, q in enumerate(example_questions, 1):\n",
        "    print(f\"   {i}. {q}\")\n",
        "\n",
        "print(\"\\nðŸ“ Usage: ask_question('Your question here')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try the first example\n",
        "ask_question(\"What is PEFT?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try another example\n",
        "ask_question(\"How to save a PEFT model?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add your own questions here!\n",
        "# ask_question(\"Your custom question about PEFT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Vector Database Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show what's in the vector database\n",
        "print(f\"ðŸ“Š Vector Database Statistics:\")\n",
        "print(f\"   â€¢ Total document chunks: {db.index.ntotal}\")\n",
        "print(f\"   â€¢ Embedding dimensions: {db.index.d}\")\n",
        "print(f\"   â€¢ Storage location: {VECTOR_DB_PATH}\")\n",
        "\n",
        "# Test similarity search\n",
        "print(\"\\nðŸ” Testing similarity search...\")\n",
        "test_query = \"adapter combination\"\n",
        "similar_docs = db.similarity_search(test_query, k=3)\n",
        "\n",
        "print(f\"\\nTop 3 most similar chunks for '{test_query}':\")\n",
        "for i, doc in enumerate(similar_docs, 1):\n",
        "    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
        "    print(f\"\\n{i}. {preview}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save current session info\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "session_info = {\n",
        "    \"created\": datetime.now().isoformat(),\n",
        "    \"documents_loaded\": len(docs),\n",
        "    \"chunks_created\": len(chunked_docs),\n",
        "    \"vector_db_size\": db.index.ntotal,\n",
        "    \"model_path\": model_path,\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "}\n",
        "\n",
        "with open(\"rag_session_info.json\", \"w\") as f:\n",
        "    json.dump(session_info, f, indent=2)\n",
        "\n",
        "print(\"ðŸ’¾ Session info saved to rag_session_info.json\")\n",
        "print(\"\\nðŸŽ‰ RAG system ready for use!\")\n",
        "print(\"\\nðŸ’¡ Tip: Next time you run this notebook, you can load the existing vector database for faster startup!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}