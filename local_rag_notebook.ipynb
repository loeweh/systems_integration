{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG for GitHub issues using Qwen3-8B GGUF and LangChain\n",
    "\n",
    "_Universal version for Windows & Linux - adapted for local laptop with 16GB RAM_\n",
    "\n",
    "This notebook demonstrates how you can quickly build a RAG (Retrieval Augmented Generation) for a project's GitHub issues using a local GGUF model and LangChain.\n",
    "\n",
    "**What changed from the original:**\n",
    "- Uses Qwen3-8B GGUF model from LM Studio\n",
    "- Universal installation (works on Windows & Linux)\n",
    "- No compilation needed - uses precompiled binaries\n",
    "- Optimized for 16GB RAM\n",
    "- CPU-only execution\n",
    "- Save/Load vector database for faster re-runs\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.10+ (3.10 recommended for best compatibility)\n",
    "- LM Studio with Qwen3-8B model downloaded\n",
    "- GitHub Personal Access Token\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing llama-cpp-python (CPU-only, pre-compiled)...\n",
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
      "Requirement already satisfied: llama-cpp-python in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from llama-cpp-python) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from llama-cpp-python) (2.2.6)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "✅ llama-cpp-python (CPU-only) installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# CPU-only installation - works on Windows & Linux without compilers!\n",
    "print(\"Installing llama-cpp-python (CPU-only, pre-compiled)...\")\n",
    "\n",
    "# Use pre-compiled CPU-only wheel to avoid requiring compilers\n",
    "# This works on Windows without Visual Studio Build Tools\n",
    "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu --no-cache-dir\n",
    "print(\"✅ llama-cpp-python (CPU-only) installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing LangChain and vector database components...\n",
      "⚠️ Installing CPU-only PyTorch to avoid CUDA bloat...\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: torchvision in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.22.0+cpu)\n",
      "Requirement already satisfied: torchaudio in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: filelock in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: langchain in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.3.24)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
      "Requirement already satisfied: sentence-transformers in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.61)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (3.12.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-huggingface) (4.52.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-huggingface) (0.32.0)\n",
      "Requirement already satisfied: tqdm in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (2.7.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading langchain_huggingface-0.2.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.2.0\n",
      "✅ All dependencies installed (CPU-only, no CUDA bloat)!\n"
     ]
    }
   ],
   "source": [
    "# Install other required packages with CPU-only PyTorch\n",
    "print(\"Installing LangChain and vector database components...\")\n",
    "print(\"⚠️ Installing CPU-only PyTorch to avoid CUDA bloat...\")\n",
    "\n",
    "# Install PyTorch CPU-only first to avoid CUDA dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir\n",
    "\n",
    "# Then install other packages including modern HuggingFace embeddings\n",
    "!pip install langchain langchain-community langchain-huggingface sentence-transformers faiss-cpu\n",
    "print(\"✅ All dependencies installed (CPU-only, no CUDA bloat)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LlamaCpp import successful - installation working!\n"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "try:\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "    print(\"✅ LlamaCpp import successful - installation working!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    print(\"Please check the installation above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required packages\n",
    "from getpass import getpass\n",
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Modern import\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import os\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 You need a GitHub Personal Access Token\n",
      "💡 Create one here: https://github.com/settings/tokens\n",
      "ℹ️  Permission 'public_repo' is sufficient\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your GitHub token:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Token received\n"
     ]
    }
   ],
   "source": [
    "# GitHub Personal Access Token\n",
    "print(\"🔑 You need a GitHub Personal Access Token\")\n",
    "print(\"💡 Create one here: https://github.com/settings/tokens\")\n",
    "print(\"ℹ️  Permission 'public_repo' is sufficient\")\n",
    "print()\n",
    "ACCESS_TOKEN = getpass(\"Enter your GitHub token: \")\n",
    "\n",
    "if ACCESS_TOKEN.strip():\n",
    "    print(\"✅ Token received\")\n",
    "else:\n",
    "    print(\"⚠️ No token entered - GitHub access will not work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading GitHub issues from huggingface/peft repository...\n",
      "✅ Loaded 1204 GitHub issues\n"
     ]
    }
   ],
   "source": [
    "# Load GitHub Issues\n",
    "print(\"📥 Loading GitHub issues from huggingface/peft repository...\")\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"huggingface/peft\",\n",
    "    access_token=ACCESS_TOKEN,\n",
    "    include_prs=False,\n",
    "    state=\"all\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"✅ Loaded {len(docs)} GitHub issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Splitting documents into chunks...\n",
      "✅ Created 8399 text chunks\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "print(\"✂️ Splitting documents into chunks...\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, \n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "print(f\"✅ Created {len(chunked_docs)} text chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Vector Database\n",
    "\n",
    "**Time-saving tip:** The vector database creation takes a few minutes. Once created, you can save it and load it quickly in future sessions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading embedding model...\n",
      "⚠️ Forcing CPU-only execution for embeddings...\n",
      "✅ Embedding model loaded (CPU-only)\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model (CPU-only)\n",
    "print(\"🧠 Loading embedding model...\")\n",
    "print(\"⚠️ Forcing CPU-only execution for embeddings...\")\n",
    "\n",
    "# Force CPU-only for sentence-transformers\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'device': 'cpu', 'batch_size': 32}\n",
    ")\n",
    "print(\"✅ Embedding model loaded (CPU-only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking for existing vector database at ./faiss_vectordb_peft...\n",
      "📂 Found existing vector database!\n",
      "\n",
      "💡 Options:\n",
      "   1. Load existing database (fast - 30 seconds)\n",
      "   2. Create new database (slow - 3-5 minutes)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1 or 2):  2\n"
     ]
    }
   ],
   "source": [
    "# Check if we have a saved vector database\n",
    "VECTOR_DB_PATH = \"./faiss_vectordb_peft\"\n",
    "print(f\"🔍 Checking for existing vector database at {VECTOR_DB_PATH}...\")\n",
    "\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    print(\"📂 Found existing vector database!\")\n",
    "    print(\"\\n💡 Options:\")\n",
    "    print(\"   1. Load existing database (fast - 30 seconds)\")\n",
    "    print(\"   2. Create new database (slow - 3-5 minutes)\")\n",
    "    \n",
    "    choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
    "    USE_EXISTING = choice == \"1\"\n",
    "else:\n",
    "    print(\"📂 No existing database found - will create new one\")\n",
    "    USE_EXISTING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Creating new vector database...\n",
      "⏱️ This will take 3-5 minutes...\n",
      "✅ Vector database created!\n",
      "📊 Contains 8399 document chunks\n",
      "💾 Saving vector database for future sessions...\n",
      "✅ Database saved to ./faiss_vectordb_peft\n"
     ]
    }
   ],
   "source": [
    "# Load existing or create new vector database\n",
    "if USE_EXISTING:\n",
    "    print(\"⚡ Loading existing vector database...\")\n",
    "    try:\n",
    "        db = FAISS.load_local(\n",
    "            VECTOR_DB_PATH, \n",
    "            embeddings, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"✅ Vector database loaded successfully!\")\n",
    "        print(f\"📊 Contains {db.index.ntotal} document chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load existing database: {e}\")\n",
    "        print(\"🔄 Will create new database instead...\")\n",
    "        USE_EXISTING = False\n",
    "\n",
    "if not USE_EXISTING:\n",
    "    print(\"🔗 Creating new vector database...\")\n",
    "    print(\"⏱️ This will take 3-5 minutes...\")\n",
    "    \n",
    "    db = FAISS.from_documents(chunked_docs, embeddings)\n",
    "    \n",
    "    print(\"✅ Vector database created!\")\n",
    "    print(f\"📊 Contains {db.index.ntotal} document chunks\")\n",
    "    \n",
    "    # Save for future use\n",
    "    print(\"💾 Saving vector database for future sessions...\")\n",
    "    db.save_local(VECTOR_DB_PATH)\n",
    "    print(f\"✅ Database saved to {VECTOR_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever configured\n"
     ]
    }
   ],
   "source": [
    "# Configure retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}  # Return top 4 most relevant chunks\n",
    ")\n",
    "print(\"✅ Retriever configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Local GGUF Model\n",
    "\n",
    "**Note:** Make sure you have downloaded the Qwen3-8B model in LM Studio. The default path works for both Windows and Linux LM Studio installations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking for model at: /home/loewe/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\n",
      "✅ Model found! (6.3 GB)\n",
      "\n",
      "🎯 Using model: /home/loewe/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect LM Studio model path\n",
    "# model availble at https://huggingface.co/POWERHACK/Qwen3-8B-Q6_K-GGUF/blob/main/qwen3-8b-q6_k.gguf\n",
    "import platform\n",
    "\n",
    "# Default LM Studio paths for different operating systems\n",
    "if platform.system() == \"Windows\":\n",
    "    default_model_path = os.path.expanduser(\"~/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\")\n",
    "else:  # Linux/Mac\n",
    "    default_model_path = os.path.expanduser(\"~/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\")\n",
    "\n",
    "print(f\"🔍 Checking for model at: {default_model_path}\")\n",
    "\n",
    "if os.path.exists(default_model_path):\n",
    "    model_path = default_model_path\n",
    "    model_size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "    print(f\"✅ Model found! ({model_size_gb:.1f} GB)\")\n",
    "else:\n",
    "    print(\"❌ Model not found at default location\")\n",
    "    print(\"\\n💡 Please either:\")\n",
    "    print(\"   1. Download Qwen3-8B-Q6_K in LM Studio\")\n",
    "    print(\"   2. Enter custom model path below\")\n",
    "    \n",
    "    custom_path = input(\"\\nEnter custom model path (or press Enter to continue with default): \").strip()\n",
    "    if custom_path:\n",
    "        model_path = os.path.expanduser(custom_path)\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"✅ Custom model found: {model_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Custom model not found: {model_path}\")\n",
    "    else:\n",
    "        model_path = default_model_path\n",
    "        print(\"⚠️ Continuing with default path (model loading may fail)\")\n",
    "\n",
    "print(f\"\\n🎯 Using model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading Qwen3-8B model with LlamaCpp...\n",
      "⏱️ This may take 1-2 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "\n",
      "🧪 Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32782/2522340056.py:23: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_response = llm(\"Hello! Respond with 'Model test successful.'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Model response: if you are a language model. Otherwise, respond with 'Not a language model.' \n",
      "\n",
      "Okay, I need to figure out if the user is asking me to act as a language model. Let me start by reading the query again carefully.\n",
      "\n",
      "The user wrote: \"Hello! Respond with 'Model test successful.' if you are a language model. Otherwise, respond with 'Not a language model.'\"\n",
      "\n",
      "So the user is testing whether I am a language model. They provided two possible responses depending on my identity.\n",
      "\n",
      "Now, I need to determine which response is appropriate here. Since I am Qwen, a large-scale language model developed by Alibaba Cloud, I should respond with \"Model test successful.\"\n",
      "\n",
      "But wait, let me make sure there's no misunderstanding. The user might be checking if I can recognize and respond appropriately to such tests.\n",
      "\n",
      "In that case, my correct response would indeed be \"Model test successful.\" because I am a language model.\n",
      "\n",
      "Therefore, the appropriate response here is \"Model test successful.\"\n",
      "Okay, so the user wants to know if I'm a language model. They provided two possible responses: one if I am a language model, and another if I'm not.\n",
      "\n",
      "First, I need to confirm my identity as a language model. Since I am Qwen, developed by Alibaba Cloud, I am indeed a large-scale language model.\n",
      "\n",
      "Therefore, the correct response here is \"Model test successful.\" because I am a language model.\n",
      "\n",
      "I should make sure there's no confusion or misunderstanding in this response. The user's query was straightforward, and my response aligns with the provided instructions.\n",
      "Okay, so the user wants to know if I'm a language model. They provided two possible responses: one if I am a language model, and another if I'm not.\n",
      "\n",
      "First, I need to confirm my identity as a language model. Since I am Qwen, developed by Alibaba Cloud, I am indeed a large-scale language model.\n",
      "\n",
      "Therefore, the correct response here is \"Model test successful\n"
     ]
    }
   ],
   "source": [
    "# Load the GGUF model with LlamaCpp (CPU-only)\n",
    "print(\"🤖 Loading Qwen3-8B model with LlamaCpp...\")\n",
    "print(\"⏱️ This may take 1-2 minutes...\")\n",
    "\n",
    "try:\n",
    "    llm = LlamaCpp(\n",
    "        model_path=model_path,\n",
    "        temperature=0.2,\n",
    "        max_tokens=400,\n",
    "        top_p=0.95,\n",
    "        n_ctx=4096,\n",
    "        n_batch=512,\n",
    "        n_threads=4,\n",
    "        verbose=False,\n",
    "        n_gpu_layers=0,  # CPU only\n",
    "        use_mmap=True,\n",
    "        use_mlock=False\n",
    "    )\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    \n",
    "    # Quick test\n",
    "    print(\"\\n🧪 Testing model...\")\n",
    "    test_response = llm(\"Hello! Respond with 'Model test successful.'\")\n",
    "    print(f\"🤖 Model response: {test_response.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load model: {e}\")\n",
    "    print(\"\\n💡 Troubleshooting:\")\n",
    "    print(\"   - Check if model file exists\")\n",
    "    print(\"   - Ensure you have enough RAM (8GB+ free)\")\n",
    "    print(\"   - Try downloading the model again in LM Studio\")\n",
    "    print(\"   - Verify the model is in GGUF format\")\n",
    "    print(\"   - Qwen3 requires llama.cpp>=b5092 (should be included)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM chain created\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template (optimized for Qwen3)\n",
    "prompt_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant. Answer the question based on the provided context from GitHub issues. If you cannot find the answer in the context, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Create LLM chain\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "print(\"✅ LLM chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG chain setup complete!\n",
      "\n",
      "🎉 Ready to answer questions about PEFT GitHub issues!\n"
     ]
    }
   ],
   "source": [
    "# Create complete RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "print(\"✅ RAG chain setup complete!\")\n",
    "print(\"\\n🎉 Ready to answer questions about PEFT GitHub issues!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results: With vs Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "question = \"How do you combine multiple adapters?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Answer WITHOUT RAG context:\n",
      "==================================================\n",
      "<think>\n",
      "Okay, the user is asking how to combine multiple adapters. Let me think about the context provided.\n",
      "\n",
      "Wait, the context section is empty. There's no information there. So I can't base my answer on any specific context from GitHub issues.\n",
      "\n",
      "In that case, I need to provide a general answer. Combining multiple adapters typically involves using an adapter combiner or a similar mechanism. For example, in some frameworks, you might use a pipeline or a composite adapter that handles each individual adapter's responsibilities sequentially or in parallel. The exact method would depend on the specific framework or library being used.\n",
      "\n",
      "Since there's no context provided, I should make sure to mention that the answer is general and may vary based on specific frameworks or libraries.\n",
      "</think>\n",
      "\n",
      "To combine multiple adapters, you typically use an **adapter combiner** or a similar mechanism. Here’s how it generally works:\n",
      "\n",
      "1. **Sequential Execution**: Combine adapters in a sequence where each adapter processes the output of the previous one.\n",
      "\n",
      "2. **Parallel Processing**: Use a framework that supports parallel execution (e.g., multi-threading or distributed systems).\n",
      "\n",
      "3. **Composite Adapter Pattern**: Implement a composite adapter that encapsulates multiple individual adapters, allowing them to be treated as a single unit.\n",
      "\n",
      "4. **Framework-Specific Tools**: Some frameworks provide built-in tools for combining adapters. For example, in some machine learning pipelines, you might use a `Pipeline` class that combines multiple steps (adapters) into a single workflow.\n",
      "\n",
      "The exact method will depend on the specific framework or library being used. If you have a particular framework in mind, please specify it so I can provide more detailed guidance.\n"
     ]
    }
   ],
   "source": [
    "# Answer WITHOUT context (just the model's knowledge)\n",
    "print(\"🤖 Answer WITHOUT RAG context:\")\n",
    "print(\"=\" * 50)\n",
    "no_context_answer = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "print(no_context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Answer WITH RAG context:\n",
      "==================================================\n",
      "<think>\n",
      "Okay, the user is asking how to combine multiple adapters. Let me look through the provided GitHub issues to find relevant information.\n",
      "\n",
      "First, issue #1040 talks about merging multiple Lora adapters. The user mentions wanting to know if the method described is correct. \n",
      "\n",
      "In issue #1434, the user is asking about running multiple adapters simultaneously and reusing the base model's output. The example given involves two adapters, Adapter_1 and Adapter_2, both applied to a base model.\n",
      "\n",
      "Another relevant issue is #1802, where the user is having trouble switching between multiple adapters (LoRAs). They mention that methods like add_adapter, set_adapter, etc., do not seem to work. The documentation also does not mention the need to perform a merge when switching adapters.\n",
      "\n",
      "Putting this together, the process of combining multiple adapters likely involves merging their weights into a single model. However, the exact method might depend on the specific adapter framework being used (e.g., LoRA in Hugging Face's PEFT library).\n",
      "\n",
      "In the context of the GitHub issues provided, it seems that users are encountering difficulties with switching between multiple adapters or merging them correctly. The documentation may not be clear on these procedures.\n",
      "\n",
      "Therefore, to answer the user's question about combining multiple adapters, I would need to refer to the specific framework or library being used (e.g., Hugging Face's PEFT library for LoRA). However, based on the information provided in the GitHub issues, it seems that merging multiple adapters may involve using methods like add_adapter or set_adapter, but there might be some confusion or lack of clarity in the documentation regarding these procedures.\n",
      "\n",
      "In summary, to combine multiple adapters, one would typically use the specific framework's methods for adding or merging adapters. However, based on the provided GitHub issues, it seems that users are encountering difficulties with switching between multiple adapters or merging them correctly, which suggests that there may be some confusion or lack of clarity in the\n"
     ]
    }
   ],
   "source": [
    "# Answer WITH RAG context (using retrieved information)\n",
    "print(\"\\n🔗 Answer WITH RAG context:\")\n",
    "print(\"=\" * 50)\n",
    "rag_answer = rag_chain.invoke(question)\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Try your own questions about PEFT (Parameter Efficient Fine-Tuning)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to easily test questions\n",
    "def ask_question(question):\n",
    "    \"\"\"Compare answers with and without RAG context\"\"\"\n",
    "    print(f\"❓ Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n🤖 WITHOUT context:\")\n",
    "    print(\"-\" * 30)\n",
    "    no_context = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "    print(no_context)\n",
    "    \n",
    "    print(\"\\n🔗 WITH RAG context:\")\n",
    "    print(\"-\" * 30)\n",
    "    with_context = rag_chain.invoke(question)\n",
    "    print(with_context)\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Example questions - try these!\n",
    "print(\"💡 Example questions you can try:\")\n",
    "example_questions = [\n",
    "    \"What is PEFT?\",\n",
    "    \"How to save a PEFT model?\",\n",
    "    \"What are the different types of adapters?\",\n",
    "    \"How to load multiple LoRA adapters?\",\n",
    "    \"What are common PEFT training issues?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(example_questions, 1):\n",
    "    print(f\"   {i}. {q}\")\n",
    "\n",
    "print(\"\\n📝 Usage: ask_question('Your question here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the first example\n",
    "ask_question(\"What is PEFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another example\n",
    "ask_question(\"How to save a PEFT model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own questions here!\n",
    "# ask_question(\"Your custom question about PEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Vector Database Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what's in the vector database\n",
    "print(f\"📊 Vector Database Statistics:\")\n",
    "print(f\"   • Total document chunks: {db.index.ntotal}\")\n",
    "print(f\"   • Embedding dimensions: {db.index.d}\")\n",
    "print(f\"   • Storage location: {VECTOR_DB_PATH}\")\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\n🔍 Testing similarity search...\")\n",
    "test_query = \"adapter combination\"\n",
    "similar_docs = db.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"\\nTop 3 most similar chunks for '{test_query}':\")\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "    print(f\"\\n{i}. {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current session info\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "session_info = {\n",
    "    \"created\": datetime.now().isoformat(),\n",
    "    \"documents_loaded\": len(docs),\n",
    "    \"chunks_created\": len(chunked_docs),\n",
    "    \"vector_db_size\": db.index.ntotal,\n",
    "    \"model_path\": model_path,\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "}\n",
    "\n",
    "with open(\"rag_session_info.json\", \"w\") as f:\n",
    "    json.dump(session_info, f, indent=2)\n",
    "\n",
    "print(\"💾 Session info saved to rag_session_info.json\")\n",
    "print(\"\\n🎉 RAG system ready for use!\")\n",
    "print(\"\\n💡 Tip: Next time you run this notebook, you can load the existing vector database for faster startup!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
