{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG for GitHub issues using Qwen3-8B GGUF and LangChain\n",
    "\n",
    "_Universal version for Windows & Linux - adapted for local laptop with 16GB RAM_\n",
    "\n",
    "This notebook demonstrates how you can quickly build a RAG (Retrieval Augmented Generation) for a project's GitHub issues using a local GGUF model and LangChain.\n",
    "\n",
    "**What changed from the original:**\n",
    "- Uses Qwen3-8B GGUF model from LM Studio\n",
    "- Universal installation (works on Windows & Linux)\n",
    "- No compilation needed - uses precompiled binaries\n",
    "- Optimized for 16GB RAM\n",
    "- CPU-only execution\n",
    "- Save/Load vector database for faster re-runs\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.10+ (3.10 recommended for best compatibility)\n",
    "- LM Studio with Qwen3-8B model downloaded\n",
    "- GitHub Personal Access Token\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing llama-cpp-python (CPU-only, no CUDA dependencies)...\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp310-cp310-linux_x86_64.whl size=4099883 sha256=5f9cfe75fa79ea3ed4c504c21157576df8f5793d0e4f6393eb0be4e3bd0f686f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sxlvgkcr/wheels/e2/91/0a/79c7b44fab10c7222ec91bd97fd7f6708beba84d5934228a80\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.13.2\n",
      "\u001b[2K    Uninstalling typing_extensions-4.13.2:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.13.2[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: numpy━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe8;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: diskcache;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: diskcache 5.6.30m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling diskcache-5.6.3:;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled diskcache-5.6.3\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: jinja28;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.6╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.6:8;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.64m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: llama-cpp-python;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: llama_cpp_python 0.3.95;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling llama_cpp_python-0.3.9:;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled llama_cpp_python-0.3.98;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [llama-cpp-python][32m5/6\u001b[0m [llama-cpp-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.9 numpy-2.2.6 typing-extensions-4.13.2\n",
      "✅ llama-cpp-python (CPU-only) installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# CPU-only installation - works on Windows & Linux!\n",
    "print(\"Installing llama-cpp-python (CPU-only, no CUDA dependencies)...\")\n",
    "\n",
    "# Force CPU-only installation to avoid CUDA bloat\n",
    "import os\n",
    "os.environ['CMAKE_ARGS'] = '-DGGML_BLAS=OFF -DGGML_CUDA=OFF -DGGML_METAL=OFF'\n",
    "os.environ['FORCE_CMAKE'] = '1'\n",
    "\n",
    "!pip install llama-cpp-python --no-cache-dir --force-reinstall --no-binary=llama-cpp-python\n",
    "print(\"✅ llama-cpp-python (CPU-only) installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing LangChain and vector database components...\n",
      "⚠️ Installing CPU-only PyTorch to avoid CUDA bloat...\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: torchvision in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.22.0+cpu)\n",
      "Requirement already satisfied: torchaudio in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: filelock in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: langchain in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.3.24)\n",
      "Requirement already satisfied: sentence-transformers in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.61)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (3.12.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (2.7.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (0.32.0)\n",
      "Requirement already satisfied: Pillow in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "✅ All dependencies installed (CPU-only, no CUDA bloat)!\n"
     ]
    }
   ],
   "source": [
    "# Install other required packages with CPU-only PyTorch\n",
    "print(\"Installing LangChain and vector database components...\")\n",
    "print(\"⚠️ Installing CPU-only PyTorch to avoid CUDA bloat...\")\n",
    "\n",
    "# Install PyTorch CPU-only first to avoid CUDA dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir\n",
    "\n",
    "# Then install other packages\n",
    "!pip install langchain langchain-community sentence-transformers faiss-cpu\n",
    "print(\"✅ All dependencies installed (CPU-only, no CUDA bloat)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LlamaCpp import successful - installation working!\n"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "try:\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "    print(\"✅ LlamaCpp import successful - installation working!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    print(\"Please check the installation above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all required packages\n",
    "from getpass import getpass\n",
    "from langchain.document_loaders import GitHubIssuesLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import os\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 You need a GitHub Personal Access Token\n",
      "💡 Create one here: https://github.com/settings/tokens\n",
      "ℹ️  Permission 'public_repo' is sufficient\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your GitHub token:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Token received\n"
     ]
    }
   ],
   "source": [
    "# GitHub Personal Access Token\n",
    "print(\"🔑 You need a GitHub Personal Access Token\")\n",
    "print(\"💡 Create one here: https://github.com/settings/tokens\")\n",
    "print(\"ℹ️  Permission 'public_repo' is sufficient\")\n",
    "print()\n",
    "ACCESS_TOKEN = getpass(\"Enter your GitHub token: \")\n",
    "\n",
    "if ACCESS_TOKEN.strip():\n",
    "    print(\"✅ Token received\")\n",
    "else:\n",
    "    print(\"⚠️ No token entered - GitHub access will not work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading GitHub issues from huggingface/peft repository...\n",
      "✅ Loaded 1204 GitHub issues\n"
     ]
    }
   ],
   "source": [
    "# Load GitHub Issues\n",
    "print(\"📥 Loading GitHub issues from huggingface/peft repository...\")\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"huggingface/peft\",\n",
    "    access_token=ACCESS_TOKEN,\n",
    "    include_prs=False,\n",
    "    state=\"all\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"✅ Loaded {len(docs)} GitHub issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Splitting documents into chunks...\n",
      "✅ Created 8399 text chunks\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "print(\"✂️ Splitting documents into chunks...\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, \n",
    "    chunk_overlap=30\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "print(f\"✅ Created {len(chunked_docs)} text chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load Vector Database\n",
    "\n",
    "**Time-saving tip:** The vector database creation takes a few minutes. Once created, you can save it and load it quickly in future sessions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7885/3974516324.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding model loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "print(\"🧠 Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "print(\"✅ Embedding model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking for existing vector database at ./faiss_vectordb_peft...\n",
      "📂 Found existing vector database!\n",
      "\n",
      "💡 Options:\n",
      "   1. Load existing database (fast - 30 seconds)\n",
      "   2. Create new database (slow - 3-5 minutes)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1 or 2):  2\n"
     ]
    }
   ],
   "source": [
    "# Check if we have a saved vector database\n",
    "VECTOR_DB_PATH = \"./faiss_vectordb_peft\"\n",
    "print(f\"🔍 Checking for existing vector database at {VECTOR_DB_PATH}...\")\n",
    "\n",
    "if os.path.exists(VECTOR_DB_PATH):\n",
    "    print(\"📂 Found existing vector database!\")\n",
    "    print(\"\\n💡 Options:\")\n",
    "    print(\"   1. Load existing database (fast - 30 seconds)\")\n",
    "    print(\"   2. Create new database (slow - 3-5 minutes)\")\n",
    "    \n",
    "    choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
    "    USE_EXISTING = choice == \"1\"\n",
    "else:\n",
    "    print(\"📂 No existing database found - will create new one\")\n",
    "    USE_EXISTING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Creating new vector database...\n",
      "⏱️ This will take 3-5 minutes...\n",
      "✅ Vector database created!\n",
      "📊 Contains 8399 document chunks\n",
      "💾 Saving vector database for future sessions...\n",
      "✅ Database saved to ./faiss_vectordb_peft\n"
     ]
    }
   ],
   "source": [
    "# Load existing or create new vector database\n",
    "if USE_EXISTING:\n",
    "    print(\"⚡ Loading existing vector database...\")\n",
    "    try:\n",
    "        db = FAISS.load_local(\n",
    "            VECTOR_DB_PATH, \n",
    "            embeddings, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"✅ Vector database loaded successfully!\")\n",
    "        print(f\"📊 Contains {db.index.ntotal} document chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load existing database: {e}\")\n",
    "        print(\"🔄 Will create new database instead...\")\n",
    "        USE_EXISTING = False\n",
    "\n",
    "if not USE_EXISTING:\n",
    "    print(\"🔗 Creating new vector database...\")\n",
    "    print(\"⏱️ This will take 3-5 minutes...\")\n",
    "    \n",
    "    db = FAISS.from_documents(chunked_docs, embeddings)\n",
    "    \n",
    "    print(\"✅ Vector database created!\")\n",
    "    print(f\"📊 Contains {db.index.ntotal} document chunks\")\n",
    "    \n",
    "    # Save for future use\n",
    "    print(\"💾 Saving vector database for future sessions...\")\n",
    "    db.save_local(VECTOR_DB_PATH)\n",
    "    print(f\"✅ Database saved to {VECTOR_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever configured\n"
     ]
    }
   ],
   "source": [
    "# Configure retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}  # Return top 4 most relevant chunks\n",
    ")\n",
    "print(\"✅ Retriever configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Local GGUF Model\n",
    "\n",
    "**Note:** Make sure you have downloaded the Qwen3-8B model in LM Studio. The default path works for both Windows and Linux LM Studio installations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking for model at: /home/loewe/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\n",
      "✅ Model found! (6.3 GB)\n",
      "\n",
      "🎯 Using model: /home/loewe/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect LM Studio model path\n",
    "import platform\n",
    "\n",
    "# Default LM Studio paths for different operating systems\n",
    "if platform.system() == \"Windows\":\n",
    "    default_model_path = os.path.expanduser(\"~/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\")\n",
    "else:  # Linux/Mac\n",
    "    default_model_path = os.path.expanduser(\"~/.lmstudio/models/lmstudio-community/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\")\n",
    "\n",
    "print(f\"🔍 Checking for model at: {default_model_path}\")\n",
    "\n",
    "if os.path.exists(default_model_path):\n",
    "    model_path = default_model_path\n",
    "    model_size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "    print(f\"✅ Model found! ({model_size_gb:.1f} GB)\")\n",
    "else:\n",
    "    print(\"❌ Model not found at default location\")\n",
    "    print(\"\\n💡 Please either:\")\n",
    "    print(\"   1. Download Qwen3-8B-Q6_K in LM Studio\")\n",
    "    print(\"   2. Enter custom model path below\")\n",
    "    \n",
    "    custom_path = input(\"\\nEnter custom model path (or press Enter to continue with default): \").strip()\n",
    "    if custom_path:\n",
    "        model_path = os.path.expanduser(custom_path)\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"✅ Custom model found: {model_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Custom model not found: {model_path}\")\n",
    "    else:\n",
    "        model_path = default_model_path\n",
    "        print(\"⚠️ Continuing with default path (model loading may fail)\")\n",
    "\n",
    "print(f\"\\n🎯 Using model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading Qwen3-8B model with LlamaCpp...\n",
      "⏱️ This may take 1-2 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "\n",
      "🧪 Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7885/2522340056.py:23: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_response = llm(\"Hello! Respond with 'Model test successful.'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Model response: if you are a language model. Otherwise, respond with 'Not a language model.' \n",
      "\n",
      "Okay, I need to figure out if the user is asking me to act as a language model. Let me start by reading the query again carefully.\n",
      "\n",
      "The user wrote: \"Hello! Respond with 'Model test successful.' if you are a language model. Otherwise, respond with 'Not a language model.'\"\n",
      "\n",
      "So the user is testing whether I am a language model. They provided two possible responses depending on my identity.\n",
      "\n",
      "Now, I need to determine which response is appropriate here. Since I am Qwen, a large-scale language model developed by Alibaba Cloud, I should respond with \"Model test successful.\"\n",
      "\n",
      "But wait, let me make sure there's no misunderstanding. The user might be checking if I can recognize and respond appropriately to such tests.\n",
      "\n",
      "In that case, my correct response would indeed be \"Model test successful.\" because I am a language model.\n",
      "\n",
      "Therefore, the appropriate response here is \"Model test successful.\"\n",
      "Okay, so the user wants to know if I'm a language model. They provided two possible responses: one if I am a language model, and another if I'm not.\n",
      "\n",
      "First, I need to confirm my identity as a language model. Since I am Qwen, developed by Alibaba Cloud, I am indeed a large-scale language model.\n",
      "\n",
      "Therefore, the correct response here is \"Model test successful.\" because I am a language model.\n",
      "\n",
      "I should make sure there's no confusion or misunderstanding in this response. The user's query was straightforward, and my response aligns with the provided instructions.\n",
      "Okay, so the user wants to know if I'm a language model. They provided two possible responses: one if I am a language model, and another if I'm not.\n",
      "\n",
      "First, I need to confirm my identity as a language model. Since I am Qwen, developed by Alibaba Cloud, I am indeed a large-scale language model.\n",
      "\n",
      "Therefore, the correct response here is \"Model test successful\n"
     ]
    }
   ],
   "source": [
    "# Load the GGUF model with LlamaCpp (CPU-only)\n",
    "print(\"🤖 Loading Qwen3-8B model with LlamaCpp...\")\n",
    "print(\"⏱️ This may take 1-2 minutes...\")\n",
    "\n",
    "try:\n",
    "    llm = LlamaCpp(\n",
    "        model_path=model_path,\n",
    "        temperature=0.2,\n",
    "        max_tokens=400,\n",
    "        top_p=0.95,\n",
    "        n_ctx=4096,\n",
    "        n_batch=512,\n",
    "        n_threads=4,\n",
    "        verbose=False,\n",
    "        n_gpu_layers=0,  # CPU only\n",
    "        use_mmap=True,\n",
    "        use_mlock=False\n",
    "    )\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    \n",
    "    # Quick test\n",
    "    print(\"\\n🧪 Testing model...\")\n",
    "    test_response = llm(\"Hello! Respond with 'Model test successful.'\")\n",
    "    print(f\"🤖 Model response: {test_response.strip()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load model: {e}\")\n",
    "    print(\"\\n💡 Troubleshooting:\")\n",
    "    print(\"   - Check if model file exists\")\n",
    "    print(\"   - Ensure you have enough RAM (8GB+ free)\")\n",
    "    print(\"   - Try downloading the model again in LM Studio\")\n",
    "    print(\"   - Verify the model is in GGUF format\")\n",
    "    print(\"   - Qwen3 requires llama.cpp>=b5092 (should be included)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM chain created\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template (optimized for Qwen3)\n",
    "prompt_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant. Answer the question based on the provided context from GitHub issues. If you cannot find the answer in the context, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Create LLM chain\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "print(\"✅ LLM chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG chain setup complete!\n",
      "\n",
      "🎉 Ready to answer questions about PEFT GitHub issues!\n"
     ]
    }
   ],
   "source": [
    "# Create complete RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "print(\"✅ RAG chain setup complete!\")\n",
    "print(\"\\n🎉 Ready to answer questions about PEFT GitHub issues!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results: With vs Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question\n",
    "question = \"How do you combine multiple adapters?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Answer WITHOUT RAG context:\n",
      "==================================================\n",
      "<think>\n",
      "Okay, the user is asking how to combine multiple adapters. Let me think about the context provided.\n",
      "\n",
      "Wait, the context section is empty. There's no information there. So I can't base my answer on any specific context from GitHub issues.\n",
      "\n",
      "In that case, I need to provide a general answer. Combining multiple adapters typically involves using an adapter combiner or a similar mechanism. For example, in some frameworks, you might use a pipeline or a composite adapter that handles each individual adapter's responsibilities sequentially or in parallel. The exact method would depend on the specific framework or library being used.\n",
      "\n",
      "Since there's no context provided, I should make sure to mention that the answer is general and may vary based on specific frameworks or libraries.\n",
      "</think>\n",
      "\n",
      "To combine multiple adapters, you typically use an **adapter combiner** or a similar mechanism. Here’s how it generally works:\n",
      "\n",
      "1. **Sequential Execution**: Combine adapters in a sequence where each adapter processes the output of the previous one.\n",
      "\n",
      "2. **Parallel Processing**: Use a framework that supports parallel execution (e.g., multi-threading or distributed systems).\n",
      "\n",
      "3. **Composite Adapter Pattern**: Implement a composite adapter that encapsulates multiple individual adapters, allowing them to be treated as a single unit.\n",
      "\n",
      "4. **Framework-Specific Tools**: Some frameworks provide built-in tools for combining adapters. For example, in some machine learning pipelines, you might use a `Pipeline` class that combines multiple steps (adapters) into a single workflow.\n",
      "\n",
      "The exact method will depend on the specific framework or library being used. If you have a particular framework in mind, please specify it so I can provide more detailed guidance.\n"
     ]
    }
   ],
   "source": [
    "# Answer WITHOUT context (just the model's knowledge)\n",
    "print(\"🤖 Answer WITHOUT RAG context:\")\n",
    "print(\"=\" * 50)\n",
    "no_context_answer = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "print(no_context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Answer WITH RAG context:\n",
      "==================================================\n",
      "<think>\n",
      "Okay, the user is asking how to combine multiple adapters. Let me look through the provided GitHub issues to find relevant information.\n",
      "\n",
      "First, there's issue #1040 titled \"Multiple Adapters merging question\". The user here is asking about merging multiple Lora adapters into the same model. The page content mentions wanting to know if the proper way to merge them is as described, and if there's a reference to follow.\n",
      "\n",
      "Another relevant issue is #1434, which talks about running multiple adapters simultaneously while reusing the output from the base model. The user here provides an example of how they would run two different adapters separately, but they're asking if it's possible to run them simultaneously.\n",
      "\n",
      "Additionally, there's issue #1802 where the user is having issues when switching between multiple adapters (LoRAs). They mention that the documentation doesn't mention the need to perform a merge when switching adapters. Also, the methods add_adapter, set_adapter, and enable_adapters do not appear to work.\n",
      "\n",
      "Putting this together, the answer should address how to combine multiple adapters, possibly referencing merging techniques like those discussed in #1040, or using methods provided by the library as mentioned in #1802.\n",
      "\n",
      "It's important to note that combining multiple adapters might involve specific steps depending on the adapter type (like LoRA) and the framework being used. The user should consult the relevant documentation or examples provided by the Hugging Face PEFT library.\n",
      "</think>\n",
      "\n",
      "To combine multiple adapters, you typically need to **merge** them into a single model. Here's how it works based on the context:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts**\n",
      "1. **Adapters**: These are modular components that modify a base model's behavior (e.g., LoRA).\n",
      "2. **Merging Adapters**: When combining multiple adapters, you need to merge their weights into the base model.\n",
      "\n",
      "---\n",
      "\n",
      "### **How to Combine Multiple Adapters**\n",
      "Based on the GitHub issues and\n"
     ]
    }
   ],
   "source": [
    "# Answer WITH RAG context (using retrieved information)\n",
    "print(\"\\n🔗 Answer WITH RAG context:\")\n",
    "print(\"=\" * 50)\n",
    "rag_answer = rag_chain.invoke(question)\n",
    "print(rag_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Try your own questions about PEFT (Parameter Efficient Fine-Tuning)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to easily test questions\n",
    "def ask_question(question):\n",
    "    \"\"\"Compare answers with and without RAG context\"\"\"\n",
    "    print(f\"❓ Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n🤖 WITHOUT context:\")\n",
    "    print(\"-\" * 30)\n",
    "    no_context = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "    print(no_context)\n",
    "    \n",
    "    print(\"\\n🔗 WITH RAG context:\")\n",
    "    print(\"-\" * 30)\n",
    "    with_context = rag_chain.invoke(question)\n",
    "    print(with_context)\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Example questions - try these!\n",
    "print(\"💡 Example questions you can try:\")\n",
    "example_questions = [\n",
    "    \"What is PEFT?\",\n",
    "    \"How to save a PEFT model?\",\n",
    "    \"What are the different types of adapters?\",\n",
    "    \"How to load multiple LoRA adapters?\",\n",
    "    \"What are common PEFT training issues?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(example_questions, 1):\n",
    "    print(f\"   {i}. {q}\")\n",
    "\n",
    "print(\"\\n📝 Usage: ask_question('Your question here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the first example\n",
    "ask_question(\"What is PEFT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another example\n",
    "ask_question(\"How to save a PEFT model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own questions here!\n",
    "# ask_question(\"Your custom question about PEFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Vector Database Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what's in the vector database\n",
    "print(f\"📊 Vector Database Statistics:\")\n",
    "print(f\"   • Total document chunks: {db.index.ntotal}\")\n",
    "print(f\"   • Embedding dimensions: {db.index.d}\")\n",
    "print(f\"   • Storage location: {VECTOR_DB_PATH}\")\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\n🔍 Testing similarity search...\")\n",
    "test_query = \"adapter combination\"\n",
    "similar_docs = db.similarity_search(test_query, k=3)\n",
    "\n",
    "print(f\"\\nTop 3 most similar chunks for '{test_query}':\")\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "    print(f\"\\n{i}. {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current session info\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "session_info = {\n",
    "    \"created\": datetime.now().isoformat(),\n",
    "    \"documents_loaded\": len(docs),\n",
    "    \"chunks_created\": len(chunked_docs),\n",
    "    \"vector_db_size\": db.index.ntotal,\n",
    "    \"model_path\": model_path,\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "}\n",
    "\n",
    "with open(\"rag_session_info.json\", \"w\") as f:\n",
    "    json.dump(session_info, f, indent=2)\n",
    "\n",
    "print(\"💾 Session info saved to rag_session_info.json\")\n",
    "print(\"\\n🎉 RAG system ready for use!\")\n",
    "print(\"\\n💡 Tip: Next time you run this notebook, you can load the existing vector database for faster startup!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
