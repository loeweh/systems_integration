{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d6e32-a67c-43d1-ad8b-55f373fd05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install C:\\Users\\hloewe\\wheels\\llama_cpp_python-0.3.9-cp310-cp310-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ccb9b-0e9b-4ad8-a2e4-ea3a3263a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "# Für den automatischen Download des Modells, falls noch nicht geschehen:\n",
    "# from huggingface_hub import hf_hub_download\n",
    "import os # Für Pfadmanagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23f64e-a130-4f71-9829-b8e72c89b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annahme: Das Modell befindet sich im Unterverzeichnis 'models'\n",
    "model_name = \"Qwen3-8B-Q6_K.gguf\"\n",
    "model_base_path = \"c:/users/hloewe/notebooks/models\" # Basisverzeichnis für Modelle\n",
    "model_path = os.path.join(model_base_path, model_name)\n",
    "\n",
    "# Sicherstellen, dass das Verzeichnis existiert\n",
    "if not os.path.exists(model_base_path):\n",
    "    os.makedirs(model_base_path)\n",
    "\n",
    "# Beispiel für den Download, falls die Datei nicht existiert (Repository-ID anpassen!)\n",
    "# if not os.path.exists(model_path):\n",
    "#     print(f\"Modell {model_name} wird heruntergeladen...\")\n",
    "#     # Ersetzen Sie <repo-id-fuer-qwen3-8b-q6k> durch die korrekte Hugging Face Repository ID\n",
    "#     # z.B. \"Qwen/Qwen1.5-7B-Chat-GGUF\" und den entsprechenden Dateinamen für Qwen3\n",
    "#     hf_hub_download(\n",
    "#         repo_id=\"<repo-id-fuer-qwen3-8b-q6k>\", # Beispiel: \"ExampleOrg/Qwen3-8B-GGUF\"\n",
    "#         filename=model_name,\n",
    "#         local_dir=model_base_path,\n",
    "#         local_dir_use_symlinks=False # Empfohlen für Windows und zur Vermeidung von Problemen\n",
    "#     )\n",
    "#     print(\"Download abgeschlossen.\")\n",
    "\n",
    "# Überprüfen, ob die Modelldatei nach dem optionalen Download existiert\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"FEHLER: Modelldatei nicht gefunden unter {model_path}. Bitte manuell herunterladen oder den Download-Pfad überprüfen.\")\n",
    "    # Hier könnte eine Exception ausgelöst oder das Skript beendet werden\n",
    "else:\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=0,      # CPU-Nutzung erzwingen [6, 7]\n",
    "        n_ctx=2048,          # Beispiel Kontextgröße [5]\n",
    "        n_threads=None,      # Threads automatisch erkennen lassen oder manuell setzen [5]\n",
    "        verbose=True         # Ausführliche Ausgabe für Diagnosezwecke\n",
    "    )\n",
    "    print(\"Llama Modell erfolgreich geladen.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
