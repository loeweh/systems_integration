{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac1586b",
   "metadata": {},
   "source": [
    "# Advanced RAG for Local PDFs using Gemini & GGUF Models\n",
    "\n",
    "This notebook demonstrates an advanced RAG (Retrieval Augmented Generation) pipeline for local PDF documents, including those with tables. It uses a local GGUF model for inference and the Gemini API for intelligent, context-aware document processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **PDF Processing:** Handles complex PDFs with text and tables using `PyMuPDF` (tables are converted to Markdown).\n",
    "- **Context-Aware Splitting:** Uses the Gemini API to split the document into semantically coherent chunks, which is superior to fixed-size splitting.\n",
    "- **High-Quality Embeddings:** Utilizes Google's `text-embedding-004` model, which aligns well with the Gemini splitter.\n",
    "- **Local LLM:** Runs inference on a local GGUF model (like Qwen, Llama, Phi-3) via `LlamaCpp` for privacy and offline use.\n",
    "- **CPU-Optimized:** Designed to run entirely on CPU with ~16GB RAM.\n",
    "- **Persistent Vector DB:** Saves and loads the vector database to accelerate subsequent sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad87e087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Installing necessary packages...\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: torchvision in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.22.0+cpu)\n",
      "Requirement already satisfied: torchaudio in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (2.7.0+cpu)\n",
      "Requirement already satisfied: filelock in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: langchain in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (0.3.24)\n",
      "Requirement already satisfied: sentence-transformers in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (1.11.0)\n",
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dotenv in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (1.1.0)\n",
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.61)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (3.12.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (2.7.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (0.32.0)\n",
      "Requirement already satisfied: Pillow in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.177.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.2-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.2.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: sympy in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: networkx in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/loewe/.conda/envs/rag/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading mmh3-5.2.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (101 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading onnxruntime-1.22.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pybase64-1.4.2-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (68 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading google_api_python_client-2.177.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=9664585979b15e7f52e56cc0b47733b8dfc05029a3f033bb98b4a490b66d3e76\n",
      "  Stored in directory: /home/loewe/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, filetype, durationpy, websockets, uvloop, uritemplate, shellingham, pyproject_hooks, pyparsing, PyMuPDF, pybase64, pyasn1, protobuf, oauthlib, mmh3, mdurl, humanfriendly, httptools, grpcio, distro, click, cachetools, bcrypt, backoff, uvicorn, rsa, requests-oauthlib, pyasn1-modules, proto-plus, posthog, opentelemetry-proto, opentelemetry-api, markdown-it-py, httplib2, googleapis-common-protos, coloredlogs, build, watchfiles, rich, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, grpcio-status, google-auth, typer, opentelemetry-sdk, kubernetes, google-auth-httplib2, google-api-core, opentelemetry-exporter-otlp-proto-grpc, google-api-python-client, google-ai-generativelanguage, chromadb, google-generativeai, langchain-google-genai\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56/56\u001b[0m [langchain-google-genai]32m53/56\u001b[0m [chromadb]tivelanguage]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyMuPDF-1.26.3 backoff-2.2.1 bcrypt-4.3.0 build-1.3.0 cachetools-5.5.2 chromadb-1.0.15 click-8.2.1 coloredlogs-15.0.1 distro-1.9.0 durationpy-0.10 filetype-1.2.0 flatbuffers-25.2.10 google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.177.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.74.0 grpcio-status-1.71.2 httplib2-0.22.0 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain-google-genai-2.0.10 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.2.0 oauthlib-3.3.1 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 posthog-5.4.0 proto-plus-1.26.1 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.2 pyparsing-3.2.3 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 rich-14.1.0 rsa-4.9.1 shellingham-1.5.4 typer-0.16.0 uritemplate-4.2.0 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0 websockets-15.0.1\n",
      "✅ llama-cpp-python is already installed.\n",
      "✅ All dependencies should now be installed.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install all required packages\n",
    "# This cell handles the installation of all necessary libraries, including the CPU-only version of PyTorch.\n",
    "print(\"🔧 Installing necessary packages...\")\n",
    "\n",
    "# First, install CPU-only PyTorch to ensure no heavy CUDA dependencies are pulled in.\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --no-cache-dir\n",
    "\n",
    "# Then, install the core components for PDF processing, embeddings, vector stores, and the Gemini API.\n",
    "!pip install langchain langchain-community sentence-transformers faiss-cpu PyMuPDF google-generativeai python-dotenv langchain-google-genai chromadb\n",
    "\n",
    "# Separate installation for llama-cpp-python, as it can be complex.\n",
    "# This tries a direct install first, which often works by pulling a pre-compiled wheel.\n",
    "try:\n",
    "    import llama_cpp\n",
    "    print(\"✅ llama-cpp-python is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Could not find llama-cpp-python, attempting to install...\")\n",
    "    # Note: On some systems, this might require build tools if a matching wheel isn't found.\n",
    "    !pip install llama-cpp-python\n",
    "\n",
    "print(\"✅ All dependencies should now be installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf127ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Setting up Google API Key...\n",
      "✅ Google API Key configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import all packages and set up the Google API Key\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import google.generativeai as genai\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print(\"🔑 Setting up Google API Key...\")\n",
    "load_dotenv() # Load variables from a .env file if it exists\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"Google API Key not found in .env file.\")\n",
    "    try:\n",
    "        GOOGLE_API_KEY = getpass(\"Please enter your Google API Key: \")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read API key: {e}\")\n",
    "        GOOGLE_API_KEY = None\n",
    "\n",
    "if GOOGLE_API_KEY:\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        print(\"✅ Google API Key configured successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to configure Google API: {e}\")\n",
    "else:\n",
    "    print(\"❌ No Google API Key provided. PDF splitting will not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f19315",
   "metadata": {},
   "source": [
    "## Step 3: PDF Processing with Gemini\n",
    "\n",
    "Here, we define two helper functions:\n",
    "1.  `extract_text_and_tables_from_pdf`: Uses `PyMuPDF` to read text and convert any tables into a Markdown format that the LLM can understand.\n",
    "2.  `split_text_with_gemini`: Sends the extracted text (including Markdown tables) to the Gemini API to be split into semantically coherent chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18af3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract text and tables from a PDF\n",
    "def extract_text_and_tables_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts text and tables (as Markdown) from a PDF file using PyMuPDF.\"\"\"\n",
    "    print(f\"📖 Reading text and tables from '{pdf_path}'...\")\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File not found at {pdf_path}\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_content = \"\"\n",
    "        for page_num, page in enumerate(doc):\n",
    "            # Find tables on the page\n",
    "            tables = page.find_tables()\n",
    "            if tables:\n",
    "                print(f\"  > Found {len(tables.tables)} table(s) on page {page_num + 1}\")\n",
    "                for table in tables:\n",
    "                    # Add a marker for the table and convert it to Markdown\n",
    "                    table_data = table.extract()\n",
    "                    if not table_data or not table_data[0]: continue\n",
    "                    markdown_table = \"| \" + \" | \".join(map(str, table_data[0])) + \" |\\n\"\n",
    "                    markdown_table += \"| \" + \" | \".join([\"---\"] * len(table_data[0])) + \" |\\n\"\n",
    "                    for row in table_data[1:]:\n",
    "                        markdown_table += \"| \" + \" | \".join(map(str, row)) + \" |\\n\"\n",
    "                    full_content += f\"\\n--- TABLE START ---\\n{markdown_table}--- TABLE END ---\\n\\n\"\n",
    "\n",
    "            # Add the plain text of the page\n",
    "            full_content += page.get_text(\"text\") + \"\\n\"\n",
    "\n",
    "        print(\"✅ Extraction of text and tables completed.\")\n",
    "        return full_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF with PyMuPDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Helper function for context-aware splitting using Gemini\n",
    "def split_text_with_gemini(text_to_split: str, model_name=\"gemini-1.5-flash\") -> list[str]:\n",
    "    \"\"\"Uses Gemini to split text into semantically coherent sections.\"\"\"\n",
    "    if not text_to_split or not GOOGLE_API_KEY:\n",
    "        print(\"❌ Cannot split text: No text provided or Google API Key is missing.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"🧠 Using {model_name} for context-aware splitting...\")\n",
    "    prompt = f\"\"\"\n",
    "    Your task is to reformat the following document text, which may include text and Markdown tables.\n",
    "    Group related sentences and ideas into coherent paragraphs or sections.\n",
    "    Each resulting section should represent a distinct topic or a continuous thought.\n",
    "    Do NOT summarize, alter, or interpret the content, only restructure it based on semantic context.\n",
    "    Preserve the Markdown tables as they are within their relevant context.\n",
    "    Separate the resulting sections with a unique delimiter: '---CHUNK_SEPARATOR---'.\n",
    "\n",
    "    Original Text:\n",
    "    ---\n",
    "    {text_to_split}\n",
    "    ---\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "        chunks = response.text.split('---CHUNK_SEPARATOR---')\n",
    "        cleaned_chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "        print(f\"✅ Text split into {len(cleaned_chunks)} semantic chunks.\")\n",
    "        return cleaned_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during communication with Gemini API: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984ead28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Reading text and tables from 'simatic_S7_300.pdf'...\n",
      "  > Found 15 table(s) on page 1\n",
      "  > Found 1 table(s) on page 2\n",
      "  > Found 0 table(s) on page 3\n",
      "  > Found 0 table(s) on page 4\n",
      "  > Found 1 table(s) on page 5\n",
      "  > Found 1 table(s) on page 6\n",
      "  > Found 0 table(s) on page 7\n",
      "  > Found 0 table(s) on page 8\n",
      "  > Found 0 table(s) on page 9\n",
      "  > Found 0 table(s) on page 10\n",
      "  > Found 0 table(s) on page 11\n",
      "  > Found 0 table(s) on page 12\n",
      "  > Found 0 table(s) on page 13\n",
      "  > Found 0 table(s) on page 14\n",
      "  > Found 0 table(s) on page 15\n",
      "  > Found 0 table(s) on page 16\n",
      "  > Found 0 table(s) on page 17\n",
      "  > Found 0 table(s) on page 18\n",
      "  > Found 0 table(s) on page 19\n",
      "  > Found 1 table(s) on page 20\n",
      "  > Found 1 table(s) on page 21\n",
      "  > Found 4 table(s) on page 22\n",
      "  > Found 1 table(s) on page 23\n",
      "  > Found 1 table(s) on page 24\n",
      "  > Found 2 table(s) on page 25\n",
      "  > Found 1 table(s) on page 26\n",
      "  > Found 1 table(s) on page 27\n",
      "  > Found 1 table(s) on page 28\n",
      "  > Found 2 table(s) on page 29\n",
      "  > Found 0 table(s) on page 30\n",
      "  > Found 2 table(s) on page 31\n",
      "  > Found 2 table(s) on page 32\n",
      "  > Found 1 table(s) on page 33\n",
      "  > Found 1 table(s) on page 34\n",
      "  > Found 2 table(s) on page 35\n",
      "  > Found 1 table(s) on page 36\n",
      "  > Found 3 table(s) on page 37\n",
      "  > Found 2 table(s) on page 38\n",
      "  > Found 1 table(s) on page 39\n",
      "  > Found 1 table(s) on page 40\n",
      "  > Found 2 table(s) on page 41\n",
      "  > Found 1 table(s) on page 42\n",
      "  > Found 1 table(s) on page 43\n",
      "  > Found 2 table(s) on page 44\n",
      "  > Found 1 table(s) on page 45\n",
      "  > Found 0 table(s) on page 46\n",
      "  > Found 2 table(s) on page 47\n",
      "  > Found 2 table(s) on page 48\n",
      "  > Found 2 table(s) on page 49\n",
      "  > Found 2 table(s) on page 50\n",
      "  > Found 2 table(s) on page 51\n",
      "  > Found 1 table(s) on page 52\n",
      "  > Found 1 table(s) on page 53\n",
      "  > Found 1 table(s) on page 54\n",
      "  > Found 0 table(s) on page 55\n",
      "  > Found 1 table(s) on page 56\n",
      "  > Found 0 table(s) on page 57\n",
      "  > Found 4 table(s) on page 58\n",
      "  > Found 0 table(s) on page 59\n",
      "  > Found 2 table(s) on page 60\n",
      "  > Found 1 table(s) on page 61\n",
      "  > Found 2 table(s) on page 62\n",
      "  > Found 4 table(s) on page 63\n",
      "  > Found 1 table(s) on page 64\n",
      "  > Found 2 table(s) on page 65\n",
      "  > Found 1 table(s) on page 66\n",
      "  > Found 1 table(s) on page 67\n",
      "  > Found 1 table(s) on page 68\n",
      "  > Found 1 table(s) on page 69\n",
      "  > Found 1 table(s) on page 70\n",
      "  > Found 2 table(s) on page 71\n",
      "  > Found 1 table(s) on page 72\n",
      "  > Found 2 table(s) on page 73\n",
      "  > Found 1 table(s) on page 74\n",
      "  > Found 0 table(s) on page 75\n",
      "  > Found 3 table(s) on page 76\n",
      "  > Found 1 table(s) on page 77\n",
      "  > Found 1 table(s) on page 78\n",
      "  > Found 1 table(s) on page 79\n",
      "  > Found 1 table(s) on page 80\n",
      "  > Found 2 table(s) on page 81\n",
      "  > Found 2 table(s) on page 82\n",
      "  > Found 2 table(s) on page 83\n",
      "  > Found 0 table(s) on page 84\n",
      "  > Found 2 table(s) on page 85\n",
      "  > Found 0 table(s) on page 86\n",
      "  > Found 5 table(s) on page 87\n",
      "  > Found 1 table(s) on page 88\n",
      "  > Found 1 table(s) on page 89\n",
      "  > Found 2 table(s) on page 90\n",
      "  > Found 2 table(s) on page 91\n",
      "  > Found 1 table(s) on page 92\n",
      "  > Found 2 table(s) on page 93\n",
      "  > Found 1 table(s) on page 94\n",
      "  > Found 1 table(s) on page 95\n",
      "  > Found 1 table(s) on page 96\n",
      "  > Found 1 table(s) on page 97\n",
      "  > Found 1 table(s) on page 98\n",
      "  > Found 2 table(s) on page 99\n",
      "  > Found 1 table(s) on page 100\n",
      "  > Found 2 table(s) on page 101\n",
      "  > Found 1 table(s) on page 102\n",
      "  > Found 0 table(s) on page 103\n",
      "  > Found 1 table(s) on page 104\n",
      "  > Found 1 table(s) on page 105\n",
      "  > Found 2 table(s) on page 106\n",
      "  > Found 1 table(s) on page 107\n",
      "  > Found 2 table(s) on page 108\n",
      "  > Found 0 table(s) on page 109\n",
      "  > Found 2 table(s) on page 110\n",
      "  > Found 1 table(s) on page 111\n",
      "  > Found 2 table(s) on page 112\n",
      "  > Found 3 table(s) on page 113\n",
      "  > Found 3 table(s) on page 114\n",
      "  > Found 1 table(s) on page 115\n",
      "  > Found 0 table(s) on page 116\n",
      "  > Found 2 table(s) on page 117\n",
      "  > Found 1 table(s) on page 118\n",
      "  > Found 1 table(s) on page 119\n",
      "  > Found 3 table(s) on page 120\n",
      "  > Found 3 table(s) on page 121\n",
      "  > Found 3 table(s) on page 122\n",
      "  > Found 1 table(s) on page 123\n",
      "  > Found 1 table(s) on page 124\n",
      "  > Found 3 table(s) on page 125\n",
      "  > Found 2 table(s) on page 126\n",
      "  > Found 1 table(s) on page 127\n",
      "  > Found 0 table(s) on page 128\n",
      "  > Found 3 table(s) on page 129\n",
      "  > Found 1 table(s) on page 130\n",
      "  > Found 1 table(s) on page 131\n",
      "  > Found 0 table(s) on page 132\n",
      "  > Found 8 table(s) on page 133\n",
      "  > Found 1 table(s) on page 134\n",
      "  > Found 1 table(s) on page 135\n",
      "  > Found 2 table(s) on page 136\n",
      "  > Found 0 table(s) on page 137\n",
      "  > Found 0 table(s) on page 138\n",
      "  > Found 0 table(s) on page 139\n",
      "  > Found 4 table(s) on page 140\n",
      "  > Found 1 table(s) on page 141\n",
      "  > Found 1 table(s) on page 142\n",
      "  > Found 3 table(s) on page 143\n",
      "  > Found 1 table(s) on page 144\n",
      "  > Found 2 table(s) on page 145\n",
      "  > Found 3 table(s) on page 146\n",
      "  > Found 1 table(s) on page 147\n",
      "  > Found 2 table(s) on page 148\n",
      "  > Found 1 table(s) on page 149\n",
      "  > Found 1 table(s) on page 150\n",
      "  > Found 3 table(s) on page 151\n",
      "  > Found 1 table(s) on page 152\n",
      "  > Found 1 table(s) on page 153\n",
      "  > Found 1 table(s) on page 154\n",
      "  > Found 1 table(s) on page 155\n",
      "  > Found 1 table(s) on page 156\n",
      "  > Found 2 table(s) on page 157\n",
      "  > Found 2 table(s) on page 158\n",
      "  > Found 2 table(s) on page 159\n",
      "  > Found 0 table(s) on page 160\n",
      "  > Found 3 table(s) on page 161\n",
      "  > Found 1 table(s) on page 162\n",
      "  > Found 1 table(s) on page 163\n",
      "  > Found 2 table(s) on page 164\n",
      "  > Found 1 table(s) on page 165\n",
      "  > Found 1 table(s) on page 166\n",
      "  > Found 1 table(s) on page 167\n",
      "  > Found 1 table(s) on page 168\n",
      "  > Found 1 table(s) on page 169\n",
      "  > Found 3 table(s) on page 170\n",
      "  > Found 1 table(s) on page 171\n",
      "  > Found 1 table(s) on page 172\n",
      "  > Found 1 table(s) on page 173\n",
      "  > Found 1 table(s) on page 174\n",
      "  > Found 0 table(s) on page 175\n",
      "  > Found 3 table(s) on page 176\n",
      "  > Found 1 table(s) on page 177\n",
      "  > Found 1 table(s) on page 178\n",
      "  > Found 0 table(s) on page 179\n",
      "  > Found 0 table(s) on page 180\n",
      "  > Found 1 table(s) on page 181\n",
      "  > Found 1 table(s) on page 182\n",
      "  > Found 1 table(s) on page 183\n",
      "  > Found 2 table(s) on page 184\n",
      "  > Found 1 table(s) on page 185\n",
      "  > Found 3 table(s) on page 186\n",
      "  > Found 1 table(s) on page 187\n",
      "  > Found 1 table(s) on page 188\n",
      "  > Found 1 table(s) on page 189\n",
      "  > Found 1 table(s) on page 190\n",
      "  > Found 2 table(s) on page 191\n",
      "  > Found 1 table(s) on page 192\n",
      "  > Found 1 table(s) on page 193\n",
      "  > Found 0 table(s) on page 194\n",
      "  > Found 2 table(s) on page 195\n",
      "  > Found 1 table(s) on page 196\n",
      "  > Found 1 table(s) on page 197\n",
      "  > Found 2 table(s) on page 198\n",
      "  > Found 1 table(s) on page 199\n",
      "  > Found 1 table(s) on page 200\n",
      "  > Found 1 table(s) on page 201\n",
      "  > Found 0 table(s) on page 202\n",
      "  > Found 0 table(s) on page 203\n",
      "  > Found 0 table(s) on page 204\n",
      "  > Found 2 table(s) on page 205\n",
      "  > Found 1 table(s) on page 206\n",
      "  > Found 3 table(s) on page 207\n",
      "  > Found 6 table(s) on page 208\n",
      "  > Found 3 table(s) on page 209\n",
      "  > Found 2 table(s) on page 210\n",
      "  > Found 1 table(s) on page 211\n",
      "  > Found 1 table(s) on page 212\n",
      "  > Found 0 table(s) on page 213\n",
      "  > Found 1 table(s) on page 214\n",
      "  > Found 1 table(s) on page 215\n",
      "  > Found 1 table(s) on page 216\n",
      "  > Found 1 table(s) on page 217\n",
      "  > Found 0 table(s) on page 218\n",
      "  > Found 0 table(s) on page 219\n",
      "  > Found 1 table(s) on page 220\n",
      "  > Found 0 table(s) on page 221\n",
      "  > Found 1 table(s) on page 222\n",
      "  > Found 2 table(s) on page 223\n",
      "  > Found 3 table(s) on page 224\n",
      "  > Found 3 table(s) on page 225\n",
      "  > Found 3 table(s) on page 226\n",
      "  > Found 3 table(s) on page 227\n",
      "  > Found 2 table(s) on page 228\n",
      "  > Found 2 table(s) on page 229\n",
      "  > Found 3 table(s) on page 230\n",
      "  > Found 3 table(s) on page 231\n",
      "  > Found 3 table(s) on page 232\n",
      "  > Found 3 table(s) on page 233\n",
      "  > Found 2 table(s) on page 234\n",
      "  > Found 2 table(s) on page 235\n",
      "  > Found 2 table(s) on page 236\n",
      "  > Found 2 table(s) on page 237\n",
      "  > Found 3 table(s) on page 238\n",
      "  > Found 3 table(s) on page 239\n",
      "  > Found 3 table(s) on page 240\n",
      "  > Found 1 table(s) on page 241\n",
      "  > Found 0 table(s) on page 242\n",
      "  > Found 0 table(s) on page 243\n",
      "  > Found 1 table(s) on page 244\n",
      "  > Found 2 table(s) on page 245\n",
      "  > Found 0 table(s) on page 246\n",
      "  > Found 0 table(s) on page 247\n",
      "  > Found 2 table(s) on page 248\n",
      "  > Found 0 table(s) on page 249\n",
      "  > Found 1 table(s) on page 250\n",
      "  > Found 0 table(s) on page 251\n",
      "  > Found 2 table(s) on page 252\n",
      "  > Found 2 table(s) on page 253\n",
      "  > Found 0 table(s) on page 254\n",
      "  > Found 1 table(s) on page 255\n",
      "  > Found 0 table(s) on page 256\n",
      "  > Found 1 table(s) on page 257\n",
      "  > Found 2 table(s) on page 258\n",
      "  > Found 1 table(s) on page 259\n",
      "  > Found 1 table(s) on page 260\n",
      "  > Found 1 table(s) on page 261\n",
      "  > Found 0 table(s) on page 262\n",
      "  > Found 2 table(s) on page 263\n",
      "  > Found 3 table(s) on page 264\n",
      "  > Found 1 table(s) on page 265\n",
      "  > Found 1 table(s) on page 266\n",
      "  > Found 2 table(s) on page 267\n",
      "  > Found 1 table(s) on page 268\n",
      "  > Found 1 table(s) on page 269\n",
      "  > Found 0 table(s) on page 270\n",
      "  > Found 0 table(s) on page 271\n",
      "  > Found 3 table(s) on page 272\n",
      "  > Found 1 table(s) on page 273\n",
      "  > Found 1 table(s) on page 274\n",
      "  > Found 2 table(s) on page 275\n",
      "  > Found 1 table(s) on page 276\n",
      "  > Found 3 table(s) on page 277\n",
      "  > Found 1 table(s) on page 278\n",
      "  > Found 1 table(s) on page 279\n",
      "  > Found 0 table(s) on page 280\n",
      "  > Found 4 table(s) on page 281\n",
      "  > Found 6 table(s) on page 282\n",
      "  > Found 1 table(s) on page 283\n",
      "  > Found 1 table(s) on page 284\n",
      "  > Found 2 table(s) on page 285\n",
      "  > Found 2 table(s) on page 286\n",
      "  > Found 1 table(s) on page 287\n",
      "  > Found 1 table(s) on page 288\n",
      "  > Found 1 table(s) on page 289\n",
      "  > Found 0 table(s) on page 290\n",
      "  > Found 1 table(s) on page 291\n",
      "  > Found 2 table(s) on page 292\n",
      "  > Found 2 table(s) on page 293\n",
      "  > Found 1 table(s) on page 294\n",
      "  > Found 1 table(s) on page 295\n",
      "  > Found 1 table(s) on page 296\n",
      "  > Found 2 table(s) on page 297\n",
      "  > Found 1 table(s) on page 298\n",
      "  > Found 3 table(s) on page 299\n",
      "  > Found 0 table(s) on page 300\n",
      "  > Found 0 table(s) on page 301\n",
      "  > Found 3 table(s) on page 302\n",
      "  > Found 3 table(s) on page 303\n",
      "  > Found 2 table(s) on page 304\n",
      "  > Found 3 table(s) on page 305\n",
      "  > Found 2 table(s) on page 306\n",
      "  > Found 1 table(s) on page 307\n",
      "  > Found 1 table(s) on page 308\n",
      "  > Found 1 table(s) on page 309\n",
      "  > Found 1 table(s) on page 310\n",
      "  > Found 2 table(s) on page 311\n",
      "  > Found 1 table(s) on page 312\n",
      "  > Found 0 table(s) on page 313\n",
      "  > Found 0 table(s) on page 314\n",
      "  > Found 3 table(s) on page 315\n",
      "  > Found 2 table(s) on page 316\n",
      "  > Found 2 table(s) on page 317\n",
      "  > Found 2 table(s) on page 318\n",
      "  > Found 1 table(s) on page 319\n",
      "  > Found 1 table(s) on page 320\n",
      "  > Found 1 table(s) on page 321\n",
      "  > Found 1 table(s) on page 322\n",
      "  > Found 1 table(s) on page 323\n",
      "  > Found 1 table(s) on page 324\n",
      "  > Found 0 table(s) on page 325\n",
      "  > Found 0 table(s) on page 326\n",
      "  > Found 0 table(s) on page 327\n",
      "  > Found 0 table(s) on page 328\n",
      "  > Found 1 table(s) on page 329\n",
      "  > Found 1 table(s) on page 330\n",
      "  > Found 1 table(s) on page 331\n",
      "  > Found 1 table(s) on page 332\n",
      "  > Found 2 table(s) on page 333\n",
      "  > Found 1 table(s) on page 334\n",
      "  > Found 2 table(s) on page 335\n",
      "  > Found 2 table(s) on page 336\n",
      "  > Found 2 table(s) on page 337\n",
      "  > Found 1 table(s) on page 338\n",
      "  > Found 0 table(s) on page 339\n",
      "  > Found 3 table(s) on page 340\n",
      "  > Found 4 table(s) on page 341\n",
      "  > Found 3 table(s) on page 342\n",
      "  > Found 1 table(s) on page 343\n",
      "  > Found 1 table(s) on page 344\n",
      "  > Found 1 table(s) on page 345\n",
      "  > Found 1 table(s) on page 346\n",
      "  > Found 2 table(s) on page 347\n",
      "  > Found 1 table(s) on page 348\n",
      "  > Found 2 table(s) on page 349\n",
      "  > Found 2 table(s) on page 350\n",
      "  > Found 2 table(s) on page 351\n",
      "  > Found 1 table(s) on page 352\n",
      "  > Found 0 table(s) on page 353\n",
      "  > Found 1 table(s) on page 354\n",
      "  > Found 1 table(s) on page 355\n",
      "  > Found 2 table(s) on page 356\n",
      "  > Found 2 table(s) on page 357\n",
      "  > Found 1 table(s) on page 358\n",
      "  > Found 1 table(s) on page 359\n",
      "  > Found 1 table(s) on page 360\n",
      "  > Found 1 table(s) on page 361\n",
      "  > Found 1 table(s) on page 362\n",
      "  > Found 1 table(s) on page 363\n",
      "  > Found 1 table(s) on page 364\n",
      "  > Found 2 table(s) on page 365\n",
      "  > Found 0 table(s) on page 366\n",
      "  > Found 0 table(s) on page 367\n",
      "  > Found 0 table(s) on page 368\n",
      "  > Found 0 table(s) on page 369\n",
      "  > Found 0 table(s) on page 370\n",
      "  > Found 0 table(s) on page 371\n",
      "  > Found 0 table(s) on page 372\n",
      "  > Found 0 table(s) on page 373\n",
      "  > Found 0 table(s) on page 374\n",
      "  > Found 0 table(s) on page 375\n",
      "  > Found 0 table(s) on page 376\n",
      "  > Found 0 table(s) on page 377\n",
      "  > Found 0 table(s) on page 378\n",
      "  > Found 2 table(s) on page 379\n",
      "  > Found 1 table(s) on page 380\n",
      "  > Found 1 table(s) on page 381\n",
      "  > Found 1 table(s) on page 382\n",
      "  > Found 2 table(s) on page 383\n",
      "  > Found 0 table(s) on page 384\n",
      "  > Found 0 table(s) on page 385\n",
      "  > Found 4 table(s) on page 386\n",
      "  > Found 5 table(s) on page 387\n",
      "  > Found 1 table(s) on page 388\n",
      "  > Found 1 table(s) on page 389\n",
      "  > Found 2 table(s) on page 390\n",
      "  > Found 1 table(s) on page 391\n",
      "  > Found 1 table(s) on page 392\n",
      "  > Found 1 table(s) on page 393\n",
      "  > Found 0 table(s) on page 394\n",
      "  > Found 1 table(s) on page 395\n",
      "  > Found 2 table(s) on page 396\n",
      "  > Found 1 table(s) on page 397\n",
      "  > Found 1 table(s) on page 398\n",
      "  > Found 2 table(s) on page 399\n",
      "  > Found 0 table(s) on page 400\n",
      "  > Found 0 table(s) on page 401\n",
      "  > Found 1 table(s) on page 402\n",
      "  > Found 3 table(s) on page 403\n",
      "  > Found 1 table(s) on page 404\n",
      "  > Found 1 table(s) on page 405\n",
      "  > Found 1 table(s) on page 406\n",
      "  > Found 1 table(s) on page 407\n",
      "  > Found 0 table(s) on page 408\n",
      "  > Found 1 table(s) on page 409\n",
      "  > Found 2 table(s) on page 410\n",
      "  > Found 1 table(s) on page 411\n",
      "  > Found 1 table(s) on page 412\n",
      "  > Found 2 table(s) on page 413\n",
      "  > Found 1 table(s) on page 414\n",
      "  > Found 0 table(s) on page 415\n",
      "  > Found 5 table(s) on page 416\n",
      "  > Found 4 table(s) on page 417\n",
      "  > Found 1 table(s) on page 418\n",
      "  > Found 1 table(s) on page 419\n",
      "  > Found 1 table(s) on page 420\n",
      "  > Found 1 table(s) on page 421\n",
      "  > Found 3 table(s) on page 422\n",
      "  > Found 0 table(s) on page 423\n",
      "  > Found 1 table(s) on page 424\n",
      "  > Found 0 table(s) on page 425\n",
      "  > Found 2 table(s) on page 426\n",
      "  > Found 1 table(s) on page 427\n",
      "  > Found 2 table(s) on page 428\n",
      "  > Found 1 table(s) on page 429\n",
      "  > Found 0 table(s) on page 430\n",
      "  > Found 1 table(s) on page 431\n",
      "  > Found 0 table(s) on page 432\n",
      "  > Found 2 table(s) on page 433\n",
      "  > Found 1 table(s) on page 434\n",
      "  > Found 2 table(s) on page 435\n",
      "  > Found 2 table(s) on page 436\n",
      "  > Found 0 table(s) on page 437\n",
      "  > Found 2 table(s) on page 438\n",
      "  > Found 1 table(s) on page 439\n",
      "  > Found 1 table(s) on page 440\n",
      "  > Found 1 table(s) on page 441\n",
      "  > Found 1 table(s) on page 442\n",
      "  > Found 3 table(s) on page 443\n",
      "  > Found 1 table(s) on page 444\n",
      "  > Found 2 table(s) on page 445\n",
      "  > Found 0 table(s) on page 446\n",
      "  > Found 1 table(s) on page 447\n",
      "  > Found 1 table(s) on page 448\n",
      "  > Found 1 table(s) on page 449\n",
      "  > Found 0 table(s) on page 450\n",
      "  > Found 1 table(s) on page 451\n",
      "  > Found 1 table(s) on page 452\n",
      "  > Found 1 table(s) on page 453\n",
      "  > Found 0 table(s) on page 454\n",
      "  > Found 1 table(s) on page 455\n",
      "  > Found 2 table(s) on page 456\n",
      "  > Found 3 table(s) on page 457\n",
      "  > Found 1 table(s) on page 458\n",
      "  > Found 0 table(s) on page 459\n",
      "  > Found 2 table(s) on page 460\n",
      "  > Found 2 table(s) on page 461\n",
      "  > Found 1 table(s) on page 462\n",
      "  > Found 2 table(s) on page 463\n",
      "  > Found 3 table(s) on page 464\n",
      "  > Found 0 table(s) on page 465\n",
      "  > Found 2 table(s) on page 466\n",
      "  > Found 1 table(s) on page 467\n",
      "  > Found 2 table(s) on page 468\n",
      "  > Found 3 table(s) on page 469\n",
      "  > Found 1 table(s) on page 470\n",
      "  > Found 2 table(s) on page 471\n",
      "  > Found 3 table(s) on page 472\n",
      "  > Found 0 table(s) on page 473\n",
      "  > Found 2 table(s) on page 474\n",
      "  > Found 1 table(s) on page 475\n",
      "  > Found 2 table(s) on page 476\n",
      "  > Found 2 table(s) on page 477\n",
      "  > Found 2 table(s) on page 478\n",
      "  > Found 1 table(s) on page 479\n",
      "  > Found 1 table(s) on page 480\n",
      "  > Found 0 table(s) on page 481\n",
      "  > Found 2 table(s) on page 482\n",
      "  > Found 3 table(s) on page 483\n",
      "  > Found 0 table(s) on page 484\n",
      "  > Found 2 table(s) on page 485\n",
      "  > Found 3 table(s) on page 486\n",
      "  > Found 1 table(s) on page 487\n",
      "  > Found 7 table(s) on page 488\n",
      "  > Found 6 table(s) on page 489\n",
      "  > Found 1 table(s) on page 490\n",
      "  > Found 2 table(s) on page 491\n",
      "  > Found 3 table(s) on page 492\n",
      "  > Found 3 table(s) on page 493\n",
      "  > Found 1 table(s) on page 494\n",
      "  > Found 0 table(s) on page 495\n",
      "  > Found 2 table(s) on page 496\n",
      "  > Found 0 table(s) on page 497\n",
      "  > Found 2 table(s) on page 498\n",
      "  > Found 0 table(s) on page 499\n",
      "  > Found 2 table(s) on page 500\n",
      "  > Found 0 table(s) on page 501\n",
      "  > Found 1 table(s) on page 502\n",
      "  > Found 2 table(s) on page 503\n",
      "  > Found 0 table(s) on page 504\n",
      "  > Found 2 table(s) on page 505\n",
      "  > Found 1 table(s) on page 506\n",
      "  > Found 4 table(s) on page 507\n",
      "  > Found 4 table(s) on page 508\n",
      "  > Found 1 table(s) on page 509\n",
      "  > Found 4 table(s) on page 510\n",
      "  > Found 1 table(s) on page 511\n",
      "  > Found 1 table(s) on page 512\n",
      "  > Found 5 table(s) on page 513\n",
      "  > Found 1 table(s) on page 514\n",
      "  > Found 1 table(s) on page 515\n",
      "  > Found 4 table(s) on page 516\n",
      "  > Found 5 table(s) on page 517\n",
      "  > Found 6 table(s) on page 518\n",
      "  > Found 2 table(s) on page 519\n",
      "  > Found 1 table(s) on page 520\n",
      "  > Found 2 table(s) on page 521\n",
      "  > Found 2 table(s) on page 522\n",
      "  > Found 2 table(s) on page 523\n",
      "  > Found 4 table(s) on page 524\n",
      "  > Found 3 table(s) on page 525\n",
      "  > Found 2 table(s) on page 526\n",
      "  > Found 3 table(s) on page 527\n",
      "  > Found 2 table(s) on page 528\n",
      "  > Found 1 table(s) on page 529\n",
      "  > Found 1 table(s) on page 530\n",
      "  > Found 1 table(s) on page 531\n",
      "  > Found 0 table(s) on page 532\n",
      "  > Found 1 table(s) on page 533\n",
      "  > Found 0 table(s) on page 534\n",
      "  > Found 1 table(s) on page 535\n",
      "  > Found 1 table(s) on page 536\n",
      "  > Found 1 table(s) on page 537\n",
      "  > Found 1 table(s) on page 538\n",
      "  > Found 1 table(s) on page 539\n",
      "  > Found 0 table(s) on page 540\n",
      "  > Found 0 table(s) on page 541\n",
      "  > Found 0 table(s) on page 542\n",
      "  > Found 0 table(s) on page 543\n",
      "  > Found 0 table(s) on page 544\n",
      "  > Found 0 table(s) on page 545\n",
      "  > Found 0 table(s) on page 546\n",
      "  > Found 0 table(s) on page 547\n",
      "  > Found 0 table(s) on page 548\n",
      "  > Found 0 table(s) on page 549\n",
      "  > Found 3 table(s) on page 550\n",
      "  > Found 5 table(s) on page 551\n",
      "  > Found 2 table(s) on page 552\n",
      "  > Found 1 table(s) on page 553\n",
      "  > Found 5 table(s) on page 554\n",
      "  > Found 5 table(s) on page 555\n",
      "  > Found 2 table(s) on page 556\n",
      "  > Found 6 table(s) on page 557\n",
      "  > Found 5 table(s) on page 558\n",
      "  > Found 15 table(s) on page 559\n",
      "  > Found 17 table(s) on page 560\n",
      "  > Found 11 table(s) on page 561\n",
      "  > Found 5 table(s) on page 562\n",
      "  > Found 2 table(s) on page 563\n",
      "✅ Extraction of text and tables completed.\n",
      "\n",
      "--- Starting Improved Splitting Process ---\n",
      "📄 Document pre-split into 117 larger chunks for processing.\n",
      "\n",
      "🧠 Processing pre-chunk 1 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 2 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 3 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 4 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 21 semantic chunks.\n",
      "  > Added 21 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 5 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 13 semantic chunks.\n",
      "  > Added 13 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 6 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 18 semantic chunks.\n",
      "  > Added 18 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 7 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 8 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 4 semantic chunks.\n",
      "  > Added 4 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 9 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 10 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 14 semantic chunks.\n",
      "  > Added 14 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 11 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 12 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 12 semantic chunks.\n",
      "  > Added 12 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 13 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 14 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 15 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 19 semantic chunks.\n",
      "  > Added 19 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 16 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 17 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 18 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 14 semantic chunks.\n",
      "  > Added 14 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 19 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 3 semantic chunks.\n",
      "  > Added 3 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 20 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 11 semantic chunks.\n",
      "  > Added 11 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 21 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 3 semantic chunks.\n",
      "  > Added 3 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 22 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 23 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 13 semantic chunks.\n",
      "  > Added 13 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 24 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 4 semantic chunks.\n",
      "  > Added 4 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 25 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 18 semantic chunks.\n",
      "  > Added 18 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 26 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 27 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 28 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 11 semantic chunks.\n",
      "  > Added 11 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 29 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 30 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 31 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 32 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 33 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 12 semantic chunks.\n",
      "  > Added 12 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 34 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 35 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 13 semantic chunks.\n",
      "  > Added 13 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 36 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 37 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 38 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 39 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 12 semantic chunks.\n",
      "  > Added 12 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 40 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 41 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 42 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 12 semantic chunks.\n",
      "  > Added 12 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 43 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 44 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 19 semantic chunks.\n",
      "  > Added 19 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 45 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 46 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 14 semantic chunks.\n",
      "  > Added 14 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 47 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 48 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 49 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 50 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 51 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 52 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 53 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 54 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 55 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 4 semantic chunks.\n",
      "  > Added 4 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 56 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 57 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 3 semantic chunks.\n",
      "  > Added 3 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 58 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 59 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 60 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 61 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 26 semantic chunks.\n",
      "  > Added 26 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 62 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 63 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 64 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 65 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 66 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 11 semantic chunks.\n",
      "  > Added 11 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 67 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 68 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 69 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 70 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 4 semantic chunks.\n",
      "  > Added 4 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 71 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 72 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 73 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 74 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 75 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 16 semantic chunks.\n",
      "  > Added 16 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 76 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 77 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 78 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 16 semantic chunks.\n",
      "  > Added 16 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 79 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 80 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 81 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 82 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 83 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 84 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 11 semantic chunks.\n",
      "  > Added 11 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 85 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 86 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 87 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 88 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 89 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 8 semantic chunks.\n",
      "  > Added 8 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 90 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 91 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 92 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 93 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 94 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 95 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 96 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 97 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 98 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 99 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 100 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 26 semantic chunks.\n",
      "  > Added 26 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 101 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 6 semantic chunks.\n",
      "  > Added 6 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 102 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 103 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 16 semantic chunks.\n",
      "  > Added 16 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 104 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 105 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 11 semantic chunks.\n",
      "  > Added 11 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 106 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 12 semantic chunks.\n",
      "  > Added 12 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 107 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 108 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 109 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 52 semantic chunks.\n",
      "  > Added 52 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 110 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 5 semantic chunks.\n",
      "  > Added 5 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 111 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 11 semantic chunks.\n",
      "  > Added 11 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 112 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 7 semantic chunks.\n",
      "  > Added 7 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 113 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 22 semantic chunks.\n",
      "  > Added 22 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 114 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 23 semantic chunks.\n",
      "  > Added 23 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 115 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 10 semantic chunks.\n",
      "  > Added 10 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 116 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 9 semantic chunks.\n",
      "  > Added 9 semantic chunks.\n",
      "\n",
      "🧠 Processing pre-chunk 117 of 117 with Gemini...\n",
      "🧠 Using gemini-1.5-flash for context-aware splitting...\n",
      "✅ Text split into 47 semantic chunks.\n",
      "  > Added 47 semantic chunks.\n",
      "\n",
      "\n",
      "✅🏁 Total semantic chunks created: 1138\n"
     ]
    }
   ],
   "source": [
    "# Du benötigst diesen Import, falls er nicht schon global vorhanden ist\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Execute the PDF processing workflow with the improved splitting strategy\n",
    "\n",
    "PDF_PATH = \"simatic_S7_300.pdf\"  # <<< IMPORTANT: SET THE PATH TO YOUR PDF FILE HERE\n",
    "\n",
    "document_text = extract_text_and_tables_from_pdf(PDF_PATH)\n",
    "context_aware_chunks = []\n",
    "\n",
    "if document_text:\n",
    "    print(\"\\n--- Starting Improved Splitting Process ---\")\n",
    "    \n",
    "    # 1. Grobes Vor-Chunking, um das API-Input-Limit zu umgehen\n",
    "    # Wir erstellen größere Chunks, die sicher in das Kontextfenster von Gemini passen.\n",
    "    # Ein Wert von 10.000 bis 15.000 ist oft ein guter Kompromiss.\n",
    "    pre_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=12000, \n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    pre_chunks = pre_splitter.split_text(document_text)\n",
    "    print(f\"📄 Document pre-split into {len(pre_chunks)} larger chunks for processing.\")\n",
    "\n",
    "    # 2. Semantisches Splitting für jeden Vor-Chunk in einer Schleife\n",
    "    final_chunks = []\n",
    "    for i, pre_chunk in enumerate(pre_chunks):\n",
    "        print(f\"\\n🧠 Processing pre-chunk {i + 1} of {len(pre_chunks)} with Gemini...\")\n",
    "        \n",
    "        # Rufe die Gemini-Funktion für jeden einzelnen Vor-Chunk auf\n",
    "        semantic_chunks_from_pre_chunk = split_text_with_gemini(pre_chunk)\n",
    "        \n",
    "        if semantic_chunks_from_pre_chunk:\n",
    "            final_chunks.extend(semantic_chunks_from_pre_chunk)\n",
    "            print(f\"  > Added {len(semantic_chunks_from_pre_chunk)} semantic chunks.\")\n",
    "        else:\n",
    "            print(\"  > No chunks returned for this pre-chunk.\")\n",
    "            # Optional: Füge den Vor-Chunk als Ganzes hinzu, wenn Gemini fehlschlägt\n",
    "            # final_chunks.append(pre_chunk) \n",
    "\n",
    "    context_aware_chunks = final_chunks\n",
    "    print(f\"\\n\\n✅🏁 Total semantic chunks created: {len(context_aware_chunks)}\")\n",
    "else:\n",
    "    print(\"⚠️ No document text to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9614ee",
   "metadata": {},
   "source": [
    "## Step 4: Create Vector Database\n",
    "\n",
    "Now we'll use Google's high-quality embedding model to convert our semantic chunks into vectors and store them in a local ChromaDB database. This database will be saved to disk to avoid re-processing in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc6be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading Google's text-embedding-004 model...\n",
      "🔗 Creating new FAISS vector database...\n",
      "⏱️ This can take a moment, depending on the number of chunks...\n",
      "💾 Saving new vector database for future sessions...\n",
      "✅ Vector database created and saved to './pdf_faiss_db' with 1138 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create or Load Vector Database with FAISS\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "\n",
    "VECTOR_DB_PATH = \"./pdf_faiss_db\"  # Wir nennen den Ordner um, um Verwechslungen zu vermeiden\n",
    "db = None\n",
    "\n",
    "if GOOGLE_API_KEY and context_aware_chunks:\n",
    "    # Lade das Google Embedding-Modell\n",
    "    print(\"🧠 Loading Google's text-embedding-004 model...\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=GOOGLE_API_KEY)\n",
    "    \n",
    "    # Prüfe, ob eine bestehende FAISS-Datenbank geladen werden kann\n",
    "    if os.path.exists(VECTOR_DB_PATH):\n",
    "        print(f\"📂 Found existing FAISS database at '{VECTOR_DB_PATH}'. Loading...\")\n",
    "        try:\n",
    "            db = FAISS.load_local(\n",
    "                folder_path=VECTOR_DB_PATH,\n",
    "                embeddings=embeddings,\n",
    "                allow_dangerous_deserialization=True # Nötig für das Laden von FAISS-Indizes\n",
    "            )\n",
    "            print(f\"✅ Vector database loaded successfully with {db.index.ntotal} chunks.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load existing database: {e}. A new one will be created.\")\n",
    "            db = None # Setze db zurück, damit eine neue DB erstellt wird\n",
    "    \n",
    "    # Erstelle eine neue Datenbank, wenn keine geladen werden konnte\n",
    "    if db is None:\n",
    "        print(f\"🔗 Creating new FAISS vector database...\")\n",
    "        print(\"⏱️ This can take a moment, depending on the number of chunks...\")\n",
    "        \n",
    "        db = FAISS.from_texts(\n",
    "            texts=context_aware_chunks, \n",
    "            embedding=embeddings\n",
    "        )\n",
    "        \n",
    "        print(\"💾 Saving new vector database for future sessions...\")\n",
    "        db.save_local(folder_path=VECTOR_DB_PATH)\n",
    "        print(f\"✅ Vector database created and saved to '{VECTOR_DB_PATH}' with {db.index.ntotal} chunks.\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Skipping vector database creation: No chunks or Google API key available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2405a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever configured.\n"
     ]
    }
   ],
   "source": [
    "# Configure the retriever\n",
    "retriever = None\n",
    "if db:\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={'k': 4}  # Return top 4 most relevant chunks\n",
    "    )\n",
    "    print(\"✅ Retriever configured.\")\n",
    "else:\n",
    "    print(\"❌ Could not configure retriever because the vector database is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c58d1",
   "metadata": {},
   "source": [
    "## Step 5: Load Local GGUF Model\n",
    "\n",
    "This section loads your local GGUF model using `LlamaCpp`. Please adjust the `model_path` to point to your model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e8b521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Attempting to load GGUF model from: models/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GGUF model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTANT: SET THE PATH TO YOUR GGUF MODEL FILE HERE ---\n",
    "# GGUF_MODEL_PATH = r\"C:\\Users\\User\\Path\\To\\Your\\Model.gguf\" # <<< WINDOWS EXAMPLE\n",
    "GGUF_MODEL_PATH = \"models/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf\" # <<< LINUX/MAC EXAMPLE\n",
    "\n",
    "llm = None\n",
    "print(f\"🤖 Attempting to load GGUF model from: {GGUF_MODEL_PATH}\")\n",
    "\n",
    "if os.path.exists(GGUF_MODEL_PATH):\n",
    "    try:\n",
    "        llm = LlamaCpp(\n",
    "            model_path=GGUF_MODEL_PATH,\n",
    "            temperature=0.2,\n",
    "            max_tokens=512,\n",
    "            top_p=0.9,\n",
    "            n_ctx=4096,\n",
    "            n_batch=512,\n",
    "            verbose=False,\n",
    "            n_gpu_layers=0  # CPU only\n",
    "        )\n",
    "        print(\"✅ GGUF model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model: {e}\")\n",
    "        llm = None\n",
    "else:\n",
    "    print(f\"❌ Model file not found at the specified path. Please update GGUF_MODEL_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349050b",
   "metadata": {},
   "source": [
    "## Step 6: Setup and Run the RAG Chain\n",
    "\n",
    "Finally, we assemble the RAG chain and test it with a question related to your PDF's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template. Adjust for your model's format if needed (e.g., Qwen3, Llama3, Phi3).\n",
    "# This template is generic but can be adapted.\n",
    "prompt_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant. Answer the user's question based only on the provided context from the document. If the answer is not in the context, clearly state that.\n",
    "Context:\n",
    "{context}<|im_end|>\n",
    "<|im_start|>user\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Create the complete RAG chain\n",
    "rag_chain = None\n",
    "if retriever and llm:\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"✅ RAG chain created and ready to use!\")\n",
    "else:\n",
    "    print(\"❌ RAG chain could not be created. Check previous steps for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13165704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG chain with a question\n",
    "if rag_chain:\n",
    "    # --- IMPORTANT: ASK A QUESTION RELEVANT TO YOUR PDF'S CONTENT ---\n",
    "    question = \"What is the main topic discussed in the document?\" # <<< CHANGE THIS QUESTION\n",
    "    \n",
    "    print(f\"\\n❓ Asking question: {question}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Invoke the chain to get an answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    print(\"💬 Answer:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"⚠️ Cannot ask question because the RAG chain was not set up correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example question\n",
    "if rag_chain:\n",
    "    question = \"Summarize the key findings from any tables in the document.\" # <<< CHANGE THIS QUESTION\n",
    "    \n",
    "    print(f\"\\n❓ Asking question: {question}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    print(\"💬 Answer:\")\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
